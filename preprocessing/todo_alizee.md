# ğŸ“‹ TODO LIST - PREPROCESSING VARIABLES ORDINALES ET CATÃ‰GORIELLES

## ğŸ¯ Objectif
CrÃ©er une classe `OrdinalPreprocessor`et `CategoricalPreprocessor` avec des mÃ©thodes indÃ©pendantes pour prÃ©processer les 96 variables ordinales et 70 variables catÃ©gorielles respectivement, en vue de prÃ©dire **MathScore** (variable cible Ã  NE PAS modifier).

---

## ğŸ“Š CONTEXTE DU DATASET

### Variables Ã  traiter :
- **96 variables ordinales** (31.2% du dataset)
  - 22 sur le contexte socio-Ã©conomique familial
  - 13 sur l'environnement de classe
  - 11 sur l'utilisation des TIC
  - 50 autres rÃ©parties sur 9 domaines

- **70 variables catÃ©gorielles** (22.7% du dataset)
  - 28 variables gÃ©nÃ©rales (mÃ©tadonnÃ©es)
  - 42 variables thÃ©matiques rÃ©parties sur 10 domaines

### Variables encodÃ©es avec haute cardinalitÃ© :
- **STRATUM** : 1316 strates (urbain/rural, public/privÃ©, rÃ©gions)
- **OCOD** : 620 codes de professions
- **CNT/CNTRYID** : 80 pays (information redondante)

---

## âœ… TODO LIST DÃ‰TAILLÃ‰E

### ğŸ”µ PHASE 1 : ANALYSE EXPLORATOIRE (EDA) - Dans Jupyter Notebook
**Objectif** : Explorer les donnÃ©es rÃ©elles pour comprendre les distributions et guider le preprocessing

#### â˜ 1.1 - CrÃ©er notebook `preprocessing/01_eda_ordinal_categorical.ipynb`
- Charger Ã©chantillon de `data/X_train.csv` (10 000-50 000 lignes)
- Charger dictionnaires de rÃ©fÃ©rence du `data/Glossaire.xlsx`

#### â˜ 1.2 - Identifier les variables ordinales et catÃ©gorielles dans les donnÃ©es rÃ©elles
- Utiliser le Glossaire.xlsx pour classifier les variables
- CrÃ©er listes: `ordinal_vars` (96 vars) et `categorical_vars` (70 vars)
- VÃ©rifier cohÃ©rence avec la structure rÃ©elle du dataset

#### â˜ 1.3 - Analyser distributions des variables ordinales
- CardinalitÃ© (nombre de valeurs uniques)
- % de valeurs manquantes par variable
- DÃ©tecter types d'Ã©chelles (Likert, frÃ©quence, quantitÃ©)
- Identifier valeurs aberrantes ou codes spÃ©ciaux (-99, 97, 98, 99)
- **Visualisations** : histogrammes, boxplots

#### â˜ 1.4 - Analyser distributions des variables catÃ©gorielles
- CardinalitÃ© par variable (faible < 10, moyenne 10-50, haute > 50)
- % de valeurs manquantes
- Identifier catÃ©gories rares (< 1% des observations)
- DÃ©sÃ©quilibre des classes (imbalance ratio)
- **Focus spÃ©cial** : STRATUM (1316), OCOD (620), CNT (80)
- **Visualisations** : barplots, treemaps pour haute cardinalitÃ©

#### â˜ 1.5 - DÃ©tecter variables redondantes
- Calculer corrÃ©lations Spearman pour paires de variables ordinales
- Calculer CramÃ©r's V pour paires de variables catÃ©gorielles
- VÃ©rifier redondance CNT vs CNTRYID
- **Output** : Liste de variables Ã  supprimer

#### â˜ 1.6 - Analyser patterns de valeurs manquantes
- Matrice de corrÃ©lation des valeurs manquantes
- Identifier si missing est informatif (MCAR, MAR, MNAR)
- DÃ©cider stratÃ©gie d'imputation par variable

#### â˜ 1.7 - Documenter conclusions EDA
- CrÃ©er rapport markdown avec dÃ©cisions de preprocessing
- Lister variables Ã  supprimer, Ã  regrouper, Ã  encoder
- DÃ©finir stratÃ©gies d'imputation par type de variable
- **Output** : `preprocessing/eda_conclusions.md`

---

### ğŸŸ¡ PHASE 2 : CRÃ‰ATION DES CLASSES DE PREPROCESSING (dans `classes/`)
**Objectif** : ImplÃ©menter les classes basÃ©es sur les conclusions de l'EDA

#### â˜ 2.1 - CrÃ©er `classes/ordinal_preprocessor.py`
- Classe `OrdinalPreprocessor` avec mÃ©thodes pour les 96 variables ordinales
- MÃ©thodes basÃ©es sur les conclusions de l'EDA (Phase 1)

#### â˜ 2.2 - CrÃ©er `classes/categorical_preprocessor.py`
- Classe `CategoricalPreprocessor` avec mÃ©thodes pour les 70 variables catÃ©gorielles
- MÃ©thodes basÃ©es sur les conclusions de l'EDA (Phase 1)

---

### ğŸŸ¢ PHASE 3 : GESTION DES VALEURS MANQUANTES (Ã  implÃ©menter dans les classes)

#### â˜ 3.1 - `flag_missing_values(df: pd.DataFrame, missing_indicators: list) -> pd.DataFrame`
**Objectif** : Identifier et harmoniser les codes de valeurs manquantes
- Codes courants PISA : -99, -98, -97, 97, 98, 99, "N/A", "Missing", ""
- Remplacer tous par np.nan
- CrÃ©er variables indicatrices si > 10% missing : `var_name_is_missing`
- **Output** : DataFrame avec valeurs manquantes harmonisÃ©es + variables indicatrices

#### â˜ 3.2 - `impute_ordinal_missing_median(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables ordinales par la mÃ©diane
- Pour variables ordinales avec < 20% missing
- Imputation par mÃ©diane (prÃ©serve le caractÃ¨re ordinal)
- Option : imputation stratifiÃ©e par pays (CNT) si pertinent
- **Output** : DataFrame avec ordinales imputÃ©es

#### â˜ 3.3 - `impute_ordinal_missing_mode(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables ordinales par le mode
- Pour variables ordinales trÃ¨s dÃ©sÃ©quilibrÃ©es
- Imputation par mode (valeur la plus frÃ©quente)
- **Output** : DataFrame avec ordinales imputÃ©es

#### â˜ 3.4 - `impute_categorical_missing_mode(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables catÃ©gorielles par le mode
- Pour variables catÃ©gorielles avec < 20% missing
- Imputation par mode global ou stratifiÃ©
- **Output** : DataFrame avec catÃ©gorielles imputÃ©es

#### â˜ 3.5 - `create_missing_category(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : CrÃ©er une catÃ©gorie "Missing" pour variables catÃ©gorielles
- Pour variables catÃ©gorielles avec > 20% missing
- Ajouter une modalitÃ© explicite "Unknown" ou "Missing"
- **Output** : DataFrame avec nouvelle catÃ©gorie

#### â˜ 3.6 - `drop_high_missing_variables(df: pd.DataFrame, threshold: float = 0.5) -> tuple`
**Objectif** : Supprimer variables avec trop de valeurs manquantes
- Identifier variables avec > 50% missing (ou seuil personnalisÃ©)
- Les exclure du dataset
- **Output** : (DataFrame nettoyÃ©, liste des variables supprimÃ©es)

---

### ğŸŸ  PHASE 4 : TRAITEMENT DES CATÃ‰GORIES RARES (Ã  implÃ©menter dans les classes)

#### â˜ 4.1 - `group_rare_categories(df: pd.DataFrame, var: str, threshold: float = 0.01) -> pd.DataFrame`
**Objectif** : Regrouper catÃ©gories rares en "Other"
- Pour une variable catÃ©gorielle
- Regrouper modalitÃ©s reprÃ©sentant < 1% (ou seuil) en "Other"
- Conserver mapping pour interprÃ©tabilitÃ©
- **Output** : DataFrame avec catÃ©gories regroupÃ©es

#### â˜ 4.2 - `reduce_stratum_dimensionality(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : RÃ©duire les 1316 strates en features exploitables
- Parser STRATUM pour extraire :
  - `stratum_location` : Urban / Rural
  - `stratum_region` : North / Center / South / etc.
  - `stratum_type` : Public / Private
  - `stratum_country` : Code pays (3 lettres)
- Supprimer STRATUM original
- **Output** : DataFrame avec 4 nouvelles variables + suppression STRATUM

#### â˜ 4.3 - `group_occupations_by_major_group(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Regrouper les 620 professions en grands groupes ISCO
- Utiliser le 1er chiffre du code OCOD pour crÃ©er 10 groupes :
  - 0: Armed forces
  - 1: Managers
  - 2: Professionals
  - 3: Technicians
  - 4: Clerical support
  - 5: Service and sales
  - 6: Skilled agricultural
  - 7: Craft workers
  - 8: Plant operators
  - 9: Elementary occupations
- **Output** : DataFrame avec OCOD remplacÃ© par OCOD_major_group

#### â˜ 4.4 - `resolve_cnt_cntryid_redundancy(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Supprimer la redondance entre CNT et CNTRYID
- VÃ©rifier corrÃ©lation parfaite
- Garder CNT (plus lisible : codes 3 lettres)
- Supprimer CNTRYID
- **Output** : DataFrame sans CNTRYID

---

### ğŸ”µ PHASE 5 : ENCODAGE DES VARIABLES (Ã  implÃ©menter dans les classes)

#### â˜ 5.1 - `encode_ordinal_variables(df: pd.DataFrame, mapping_dict: dict = None) -> pd.DataFrame`
**Objectif** : Encoder les variables ordinales en prÃ©servant l'ordre
- Utiliser OrdinalEncoder de sklearn
- CrÃ©er mappings explicites pour Ã©chelles Likert, frÃ©quences
- Exemple : {"Never": 0, "Rarely": 1, "Sometimes": 2, "Often": 3, "Always": 4}
- Stocker encoders dans `self.encoders`
- **Output** : DataFrame avec ordinales encodÃ©es en int

#### â˜ 5.2 - `encode_binary_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Encoder variables catÃ©gorielles binaires
- Pour variables avec exactement 2 modalitÃ©s (ex: Gender, OECD Yes/No)
- Encoder en 0/1 avec LabelEncoder
- **Output** : DataFrame avec binaires encodÃ©es

#### â˜ 5.3 - `onehot_encode_low_cardinality(df: pd.DataFrame, max_categories: int = 10) -> pd.DataFrame`
**Objectif** : One-Hot Encoding pour variables Ã  faible cardinalitÃ©
- Pour variables catÃ©gorielles avec â‰¤ 10 modalitÃ©s
- Utiliser pd.get_dummies ou OneHotEncoder
- Nommer colonnes : `var_name_category`
- **Output** : DataFrame avec colonnes one-hot crÃ©Ã©es

#### â˜ 5.4 - `target_encode_high_cardinality(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Target Encoding pour variables Ã  haute cardinalitÃ©
- Pour variables avec > 10 modalitÃ©s (CNT, langues, etc.)
- Encoder par moyenne de MathScore pour chaque catÃ©gorie
- Ajouter rÃ©gularisation (smoothing) pour catÃ©gories rares
- Attention au data leakage : utiliser cross-validation
- **Output** : DataFrame avec target encoding appliquÃ©

#### â˜ 5.5 - `frequency_encode_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Frequency Encoding (alternative au Target Encoding)
- Encoder par frÃ©quence d'apparition de chaque catÃ©gorie
- Moins risquÃ© que target encoding (pas de leakage)
- **Output** : DataFrame avec frequency encoding appliquÃ©

---

### ğŸ”´ PHASE 6 : VALIDATION ET CONTRÃ”LE QUALITÃ‰ (Ã  implÃ©menter dans les classes)

#### â˜ 6.1 - `validate_no_missing_after_preprocessing(df: pd.DataFrame) -> bool`
**Objectif** : VÃ©rifier qu'il n'y a plus de valeurs manquantes
- Compter les NaN restants
- Lever une exception si NaN dÃ©tectÃ©s
- **Output** : True si OK, raise ValueError sinon

#### â˜ 6.2 - `validate_dtypes_after_encoding(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : VÃ©rifier les types de donnÃ©es aprÃ¨s encodage
- Ordinales encodÃ©es â†’ int ou float
- CatÃ©gorielles encodÃ©es â†’ int ou float
- Pas de type 'object' sauf si voulu
- **Output** : DataFrame de validation avec [column, expected_dtype, actual_dtype, status]

#### â˜ 6.3 - `check_target_variable_unchanged(df_before: pd.DataFrame, df_after: pd.DataFrame) -> bool`
**Objectif** : VÃ©rifier que MathScore n'a pas Ã©tÃ© modifiÃ©
- Comparer MathScore avant et aprÃ¨s preprocessing
- Lever exception si diffÃ©rences dÃ©tectÃ©es
- **Output** : True si identique, raise ValueError sinon

#### â˜ 6.4 - `generate_preprocessing_report(df_before: pd.DataFrame, df_after: pd.DataFrame) -> dict`
**Objectif** : GÃ©nÃ©rer un rapport de preprocessing
- Nombre de variables avant/aprÃ¨s
- Variables supprimÃ©es et raison
- Variables crÃ©Ã©es (one-hot, indicatrices missing)
- Statistiques d'encodage
- **Output** : Dict avec toutes les mÃ©tadonnÃ©es

#### â˜ 6.5 - `detect_data_leakage_risk(df: pd.DataFrame) -> list`
**Objectif** : DÃ©tecter les risques de data leakage
- Identifier si target encoding fait sans CV
- Identifier si imputation utilise statistiques globales
- Identifier si normalisation faite sur tout le dataset
- **Output** : Liste des warnings de leakage potentiel

---

### ğŸŸ£ PHASE 7 : PIPELINE ET SAUVEGARDE (Ã  implÃ©menter dans les classes)

#### â˜ 7.1 - `create_preprocessing_pipeline(steps: list) -> Pipeline`
**Objectif** : CrÃ©er un pipeline sklearn rÃ©utilisable
- EnchaÃ®ner les transformations dans l'ordre
- Utiliser ColumnTransformer pour appliquer transformations par type
- **Output** : Pipeline sklearn fitted

#### â˜ 7.2 - `save_encoders_and_mappings(filepath: str) -> None`
**Objectif** : Sauvegarder les encoders pour rÃ©utilisation
- Pickler les OrdinalEncoder, LabelEncoder, OneHotEncoder
- Sauvegarder les mappings de rÃ©fÃ©rence
- Sauvegarder les listes de variables par type
- **Output** : Fichier .pkl

#### â˜ 7.3 - `export_preprocessed_data(df: pd.DataFrame, filepath: str) -> None`
**Objectif** : Exporter le dataset prÃ©processÃ©
- Sauvegarder en CSV ou Parquet
- Inclure mÃ©tadonnÃ©es dans un fichier sÃ©parÃ©
- **Output** : Fichiers data + metadata

#### â˜ 7.4 - `transform_new_data(df_new: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Appliquer le preprocessing Ã  de nouvelles donnÃ©es
- Charger les encoders sauvegardÃ©s
- Appliquer les mÃªmes transformations
- GÃ©rer les nouvelles catÃ©gories inconnues
- **Output** : DataFrame transformÃ©

---

## ğŸ¯ ORDRE DE PRIORITÃ‰ D'EXÃ‰CUTION

### Phase 1 - EDA dans Jupyter Notebook
1.1 â†’ 1.2 â†’ 1.3 â†’ 1.4 â†’ 1.5 â†’ 1.6 â†’ 1.7

### Phase 2 - CrÃ©er les classes
2.1 (OrdinalPreprocessor) + 2.2 (CategoricalPreprocessor)

### Phase 3 - Gestion valeurs manquantes
3.1 â†’ 3.2 â†’ 3.3 â†’ 3.4 â†’ 3.5 â†’ 3.6

### Phase 4 - Traitement catÃ©gories rares
4.1 â†’ 4.2 â†’ 4.3 â†’ 4.4

### Phase 5 - Encodage
5.1 â†’ 5.2 â†’ 5.3 â†’ (5.4 OU 5.5)

### Phase 6 - Validation
6.1 â†’ 6.2 â†’ 6.3 â†’ 6.4 â†’ 6.5

### Phase 7 - Pipeline et sauvegarde
7.1 â†’ 7.2 â†’ 7.3 â†’ 7.4

---

## âš ï¸ POINTS D'ATTENTION CRITIQUES

### ğŸš¨ Data Leakage
- **Target encoding** : OBLIGATOIRE d'utiliser cross-validation
- **Imputation** : Calculer statistiques UNIQUEMENT sur train set
- **Scaling** : Fit sur train, transform sur test

### ğŸš¨ Gestion de MathScore (Cible)
- **NE JAMAIS** modifier, imputer, ou encoder MathScore
- VÃ©rifier aprÃ¨s chaque transformation avec `check_target_variable_unchanged()`
- Exclure MathScore de toutes les transformations

### ğŸš¨ Haute CardinalitÃ©
- STRATUM (1316) et OCOD (620) : **RÃ‰DUCTION OBLIGATOIRE**
- Ne JAMAIS faire de one-hot sur ces variables
- PrivilÃ©gier feature engineering intelligent

### ğŸš¨ Variables Redondantes
- CNT vs CNTRYID : supprimer l'un des deux
- VÃ©rifier corrÃ©lations avant encodage

### ğŸš¨ PrÃ©servation de l'Information Ordinale
- Ne JAMAIS one-hot des variables ordinales
- Utiliser OrdinalEncoder avec mapping explicite
- Documenter l'ordre des modalitÃ©s

---

## ğŸ“š LIBRAIRIES NÃ‰CESSAIRES

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from category_encoders import TargetEncoder
import pickle
import json
```

---

## ğŸ“Š LIVRABLES ATTENDUS

1. **Classe Python** : `OrdinalCategoricalPreprocessor` avec toutes les mÃ©thodes
2. **Notebook d'exemples** : DÃ©monstration de chaque mÃ©thode
3. **Dataset preprocessÃ©** : Fichier final prÃªt pour modÃ©lisation
4. **Documentation** : Rapport de preprocessing dÃ©taillÃ©
5. **Encoders sauvegardÃ©s** : Fichiers .pkl pour rÃ©utilisation
6. **Tests unitaires** : Validation de chaque mÃ©thode

---

## ğŸ“ BONNES PRATIQUES Ã€ RESPECTER

âœ… **Programmation OrientÃ©e Objet** : CrÃ©er des classes si c'est pertinent.
âœ… **Noms de fonctions explicites** : `encode_ordinal_variables` pas `encode_vars`
âœ… **Docstrings complÃ¨tes** : ParamÃ¨tres, returns, exemples
âœ… **Type hints** : `def func(df: pd.DataFrame) -> pd.DataFrame`
âœ… **Logging** : Logger chaque transformation importante
âœ… **TraÃ§abilitÃ©** : Conserver metadata de chaque transformation
âœ… **Tests** : Valider sur Ã©chantillon avant full dataset
âœ… **ModularitÃ©** : Chaque fonction fait UNE chose
âœ… **RÃ©utilisabilitÃ©** : Code applicable Ã  de nouvelles donnÃ©es

---

## ğŸ“ NOTES FINALES

- Cette TODO list est **exhaustive mais flexible** : adapter selon les donnÃ©es rÃ©elles
- Certaines mÃ©thodes peuvent Ãªtre optionnelles selon les analyses de Phase 1
- Prioriser la **qualitÃ©** sur la vitesse : un bon preprocessing = 80% du succÃ¨s du modÃ¨le
- **Documenter** toutes les dÃ©cisions prises et les justifier

**PrÃªt Ã  commencer le dÃ©veloppement ! ğŸš€**