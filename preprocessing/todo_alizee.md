# ğŸ“‹ TODO LIST - PREPROCESSING VARIABLES ORDINALES ET CATÃ‰GORIELLES

## ğŸ¯ Objectif
CrÃ©er une classe `OrdinalPreprocessor`et `CategoricalPreprocessor` avec des mÃ©thodes indÃ©pendantes pour prÃ©processer les 96 variables ordinales et 70 variables catÃ©gorielles respectivement, en vue de prÃ©dire **MathScore** (variable cible Ã  NE PAS modifier).

---

## ğŸ“Š CONTEXTE DU DATASET

### Variables Ã  traiter :
- **96 variables ordinales** (31.2% du dataset)
  - 22 sur le contexte socio-Ã©conomique familial
  - 13 sur l'environnement de classe
  - 11 sur l'utilisation des TIC
  - 50 autres rÃ©parties sur 9 domaines

- **70 variables catÃ©gorielles** (22.7% du dataset)
  - 28 variables gÃ©nÃ©rales (mÃ©tadonnÃ©es)
  - 42 variables thÃ©matiques rÃ©parties sur 10 domaines

### Variables encodÃ©es avec haute cardinalitÃ© :
- **STRATUM** : 1316 strates (urbain/rural, public/privÃ©, rÃ©gions)
- **OCOD** : 620 codes de professions
- **CNT/CNTRYID** : 80 pays (information redondante)

---

## âœ… TODO LIST DÃ‰TAILLÃ‰E

### ğŸ”µ PHASE 1 : ANALYSE EXPLORATOIRE (EDA)

#### â˜ 1.1 - `load_reference_dictionaries(glossaire_path: str) -> dict`
**Objectif** : Charger les 5 feuilles de rÃ©fÃ©rence du glossaire
- Charger CNT, CNTRYID, STRATUM, ISCEDP, OCOD
- CrÃ©er des dictionnaires de mapping {code: label}
- Stocker dans `self.reference_dicts`
- **Output** : Dict avec clÃ©s ['CNT', 'CNTRYID', 'STRATUM', 'ISCEDP', 'OCOD']

#### â˜ 1.2 - `analyze_ordinal_distributions(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Analyser les distributions des 96 variables ordinales
- Pour chaque variable ordinale :
  - Compter les modalitÃ©s uniques
  - Calculer le % de valeurs manquantes
  - Identifier les modalitÃ©s les plus frÃ©quentes
  - DÃ©tecter les valeurs aberrantes (hors plage attendue)
- **Output** : DataFrame avec colonnes [variable, n_unique, missing_%, top_3_values, potential_issues]

#### â˜ 1.3 - `analyze_categorical_distributions(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Analyser les distributions des 70 variables catÃ©gorielles
- Pour chaque variable catÃ©gorielle :
  - Compter la cardinalitÃ© (nombre de catÃ©gories)
  - Calculer le % de valeurs manquantes
  - Identifier les catÃ©gories rares (< 1% ou seuil personnalisÃ©)
  - Mesurer le dÃ©sÃ©quilibre des classes
- **Output** : DataFrame avec colonnes [variable, cardinality, missing_%, rare_categories, imbalance_ratio]

#### â˜ 1.4 - `identify_redundant_variables(df: pd.DataFrame, threshold: float = 0.95) -> list`
**Objectif** : DÃ©tecter les variables redondantes
- Calculer corrÃ©lations de Spearman pour ordinales
- Calculer CramÃ©r's V pour catÃ©gorielles
- Identifier CNT vs CNTRYID (100% redondants)
- **Output** : Liste de tuples [(var1, var2, correlation_score)]

#### â˜ 1.5 - `detect_ordinal_scales(df: pd.DataFrame) -> dict`
**Objectif** : Identifier les types d'Ã©chelles ordinales
- DÃ©tecter Ã©chelles de Likert (3, 4, 5, 7 points)
- DÃ©tecter Ã©chelles de frÃ©quence (Never, Rarely, Sometimes, Often, Always)
- DÃ©tecter Ã©chelles de quantitÃ© (0, 1-2, 3-5, 6-10, 11+)
- **Output** : Dict {variable: scale_type, ex: 'likert_5', 'frequency_6', 'quantity_ranges'}

---

### ğŸŸ¡ PHASE 2 : GESTION DES VALEURS MANQUANTES

#### â˜ 2.1 - `flag_missing_values(df: pd.DataFrame, missing_indicators: list) -> pd.DataFrame`
**Objectif** : Identifier et harmoniser les codes de valeurs manquantes
- Codes courants PISA : -99, -98, -97, 97, 98, 99, "N/A", "Missing", ""
- Remplacer tous par np.nan
- CrÃ©er variables indicatrices si > 10% missing : `var_name_is_missing`
- **Output** : DataFrame avec valeurs manquantes harmonisÃ©es + variables indicatrices

#### â˜ 2.2 - `impute_ordinal_missing_median(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables ordinales par la mÃ©diane
- Pour variables ordinales avec < 20% missing
- Imputation par mÃ©diane (prÃ©serve le caractÃ¨re ordinal)
- Option : imputation stratifiÃ©e par pays (CNT) si pertinent
- **Output** : DataFrame avec ordinales imputÃ©es

#### â˜ 2.3 - `impute_ordinal_missing_mode(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables ordinales par le mode
- Pour variables ordinales trÃ¨s dÃ©sÃ©quilibrÃ©es
- Imputation par mode (valeur la plus frÃ©quente)
- **Output** : DataFrame avec ordinales imputÃ©es

#### â˜ 2.4 - `impute_categorical_missing_mode(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Imputer variables catÃ©gorielles par le mode
- Pour variables catÃ©gorielles avec < 20% missing
- Imputation par mode global ou stratifiÃ©
- **Output** : DataFrame avec catÃ©gorielles imputÃ©es

#### â˜ 2.5 - `create_missing_category(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : CrÃ©er une catÃ©gorie "Missing" pour variables catÃ©gorielles
- Pour variables catÃ©gorielles avec > 20% missing
- Ajouter une modalitÃ© explicite "Unknown" ou "Missing"
- **Output** : DataFrame avec nouvelle catÃ©gorie

#### â˜ 2.6 - `drop_high_missing_variables(df: pd.DataFrame, threshold: float = 0.5) -> tuple`
**Objectif** : Supprimer variables avec trop de valeurs manquantes
- Identifier variables avec > 50% missing (ou seuil personnalisÃ©)
- Les exclure du dataset
- **Output** : (DataFrame nettoyÃ©, liste des variables supprimÃ©es)

---

### ğŸŸ¢ PHASE 3 : TRAITEMENT DES CATÃ‰GORIES RARES

#### â˜ 3.1 - `group_rare_categories(df: pd.DataFrame, var: str, threshold: float = 0.01) -> pd.DataFrame`
**Objectif** : Regrouper catÃ©gories rares en "Other"
- Pour une variable catÃ©gorielle
- Regrouper modalitÃ©s reprÃ©sentant < 1% (ou seuil) en "Other"
- Conserver mapping pour interprÃ©tabilitÃ©
- **Output** : DataFrame avec catÃ©gories regroupÃ©es

#### â˜ 3.2 - `reduce_stratum_dimensionality(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : RÃ©duire les 1316 strates en features exploitables
- Parser STRATUM pour extraire :
  - `stratum_location` : Urban / Rural
  - `stratum_region` : North / Center / South / etc.
  - `stratum_type` : Public / Private
  - `stratum_country` : Code pays (3 lettres)
- Supprimer STRATUM original
- **Output** : DataFrame avec 4 nouvelles variables + suppression STRATUM

#### â˜ 3.3 - `group_occupations_by_major_group(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Regrouper les 620 professions en grands groupes ISCO
- Utiliser le 1er chiffre du code OCOD pour crÃ©er 10 groupes :
  - 0: Armed forces
  - 1: Managers
  - 2: Professionals
  - 3: Technicians
  - 4: Clerical support
  - 5: Service and sales
  - 6: Skilled agricultural
  - 7: Craft workers
  - 8: Plant operators
  - 9: Elementary occupations
- **Output** : DataFrame avec OCOD remplacÃ© par OCOD_major_group

#### â˜ 3.4 - `resolve_cnt_cntryid_redundancy(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Supprimer la redondance entre CNT et CNTRYID
- VÃ©rifier corrÃ©lation parfaite
- Garder CNT (plus lisible : codes 3 lettres)
- Supprimer CNTRYID
- **Output** : DataFrame sans CNTRYID

---

### ğŸŸ  PHASE 4 : ENCODAGE DES VARIABLES

#### â˜ 4.1 - `encode_ordinal_variables(df: pd.DataFrame, mapping_dict: dict = None) -> pd.DataFrame`
**Objectif** : Encoder les variables ordinales en prÃ©servant l'ordre
- Utiliser OrdinalEncoder de sklearn
- CrÃ©er mappings explicites pour Ã©chelles Likert, frÃ©quences
- Exemple : {"Never": 0, "Rarely": 1, "Sometimes": 2, "Often": 3, "Always": 4}
- Stocker encoders dans `self.encoders`
- **Output** : DataFrame avec ordinales encodÃ©es en int

#### â˜ 4.2 - `encode_binary_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Encoder variables catÃ©gorielles binaires
- Pour variables avec exactement 2 modalitÃ©s (ex: Gender, OECD Yes/No)
- Encoder en 0/1 avec LabelEncoder
- **Output** : DataFrame avec binaires encodÃ©es

#### â˜ 4.3 - `onehot_encode_low_cardinality(df: pd.DataFrame, max_categories: int = 10) -> pd.DataFrame`
**Objectif** : One-Hot Encoding pour variables Ã  faible cardinalitÃ©
- Pour variables catÃ©gorielles avec â‰¤ 10 modalitÃ©s
- Utiliser pd.get_dummies ou OneHotEncoder
- Nommer colonnes : `var_name_category`
- **Output** : DataFrame avec colonnes one-hot crÃ©Ã©es

#### â˜ 4.4 - `target_encode_high_cardinality(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Target Encoding pour variables Ã  haute cardinalitÃ©
- Pour variables avec > 10 modalitÃ©s (CNT, langues, etc.)
- Encoder par moyenne de MathScore pour chaque catÃ©gorie
- Ajouter rÃ©gularisation (smoothing) pour catÃ©gories rares
- Attention au data leakage : utiliser cross-validation
- **Output** : DataFrame avec target encoding appliquÃ©

#### â˜ 4.5 - `frequency_encode_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Frequency Encoding (alternative au Target Encoding)
- Encoder par frÃ©quence d'apparition de chaque catÃ©gorie
- Moins risquÃ© que target encoding (pas de leakage)
- **Output** : DataFrame avec frequency encoding appliquÃ©

---

### ğŸ”´ PHASE 5 : VALIDATION ET CONTRÃ”LE QUALITÃ‰

#### â˜ 5.1 - `validate_no_missing_after_preprocessing(df: pd.DataFrame) -> bool`
**Objectif** : VÃ©rifier qu'il n'y a plus de valeurs manquantes
- Compter les NaN restants
- Lever une exception si NaN dÃ©tectÃ©s
- **Output** : True si OK, raise ValueError sinon

#### â˜ 5.2 - `validate_dtypes_after_encoding(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : VÃ©rifier les types de donnÃ©es aprÃ¨s encodage
- Ordinales encodÃ©es â†’ int ou float
- CatÃ©gorielles encodÃ©es â†’ int ou float
- Pas de type 'object' sauf si voulu
- **Output** : DataFrame de validation avec [column, expected_dtype, actual_dtype, status]

#### â˜ 5.3 - `check_target_variable_unchanged(df_before: pd.DataFrame, df_after: pd.DataFrame) -> bool`
**Objectif** : VÃ©rifier que MathScore n'a pas Ã©tÃ© modifiÃ©
- Comparer MathScore avant et aprÃ¨s preprocessing
- Lever exception si diffÃ©rences dÃ©tectÃ©es
- **Output** : True si identique, raise ValueError sinon

#### â˜ 5.4 - `generate_preprocessing_report(df_before: pd.DataFrame, df_after: pd.DataFrame) -> dict`
**Objectif** : GÃ©nÃ©rer un rapport de preprocessing
- Nombre de variables avant/aprÃ¨s
- Variables supprimÃ©es et raison
- Variables crÃ©Ã©es (one-hot, indicatrices missing)
- Statistiques d'encodage
- **Output** : Dict avec toutes les mÃ©tadonnÃ©es

#### â˜ 5.5 - `detect_data_leakage_risk(df: pd.DataFrame) -> list`
**Objectif** : DÃ©tecter les risques de data leakage
- Identifier si target encoding fait sans CV
- Identifier si imputation utilise statistiques globales
- Identifier si normalisation faite sur tout le dataset
- **Output** : Liste des warnings de leakage potentiel

---

### ğŸŸ£ PHASE 6 : PIPELINE ET SAUVEGARDE

#### â˜ 6.1 - `create_preprocessing_pipeline(steps: list) -> Pipeline`
**Objectif** : CrÃ©er un pipeline sklearn rÃ©utilisable
- EnchaÃ®ner les transformations dans l'ordre
- Utiliser ColumnTransformer pour appliquer transformations par type
- **Output** : Pipeline sklearn fitted

#### â˜ 6.2 - `save_encoders_and_mappings(filepath: str) -> None`
**Objectif** : Sauvegarder les encoders pour rÃ©utilisation
- Pickler les OrdinalEncoder, LabelEncoder, OneHotEncoder
- Sauvegarder les mappings de rÃ©fÃ©rence
- Sauvegarder les listes de variables par type
- **Output** : Fichier .pkl

#### â˜ 6.3 - `export_preprocessed_data(df: pd.DataFrame, filepath: str) -> None`
**Objectif** : Exporter le dataset prÃ©processÃ©
- Sauvegarder en CSV ou Parquet
- Inclure mÃ©tadonnÃ©es dans un fichier sÃ©parÃ©
- **Output** : Fichiers data + metadata

#### â˜ 6.4 - `transform_new_data(df_new: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Appliquer le preprocessing Ã  de nouvelles donnÃ©es
- Charger les encoders sauvegardÃ©s
- Appliquer les mÃªmes transformations
- GÃ©rer les nouvelles catÃ©gories inconnues
- **Output** : DataFrame transformÃ©

---

## ğŸ¯ ORDRE DE PRIORITÃ‰ D'EXÃ‰CUTION

### Sprint 1 - Analyse (Semaine 1)
1. 1.1 â†’ 1.2 â†’ 1.3 â†’ 1.4 â†’ 1.5

### Sprint 2 - Nettoyage (Semaine 1-2)
2. 2.1 â†’ 2.2 â†’ 2.3 â†’ 2.4 â†’ 2.5 â†’ 2.6

### Sprint 3 - RÃ©duction (Semaine 2)
3. 3.1 â†’ 3.2 â†’ 3.3 â†’ 3.4

### Sprint 4 - Encodage (Semaine 2-3)
4. 4.1 â†’ 4.2 â†’ 4.3 â†’ (4.4 OU 4.5)

### Sprint 5 - Validation (Semaine 3)
5. 5.1 â†’ 5.2 â†’ 5.3 â†’ 5.4 â†’ 5.5

### Sprint 6 - Production (Semaine 3)
6. 6.1 â†’ 6.2 â†’ 6.3 â†’ 6.4

---

## âš ï¸ POINTS D'ATTENTION CRITIQUES

### ğŸš¨ Data Leakage
- **Target encoding** : OBLIGATOIRE d'utiliser cross-validation
- **Imputation** : Calculer statistiques UNIQUEMENT sur train set
- **Scaling** : Fit sur train, transform sur test

### ğŸš¨ Gestion de MathScore (Cible)
- **NE JAMAIS** modifier, imputer, ou encoder MathScore
- VÃ©rifier aprÃ¨s chaque transformation avec `check_target_variable_unchanged()`
- Exclure MathScore de toutes les transformations

### ğŸš¨ Haute CardinalitÃ©
- STRATUM (1316) et OCOD (620) : **RÃ‰DUCTION OBLIGATOIRE**
- Ne JAMAIS faire de one-hot sur ces variables
- PrivilÃ©gier feature engineering intelligent

### ğŸš¨ Variables Redondantes
- CNT vs CNTRYID : supprimer l'un des deux
- VÃ©rifier corrÃ©lations avant encodage

### ğŸš¨ PrÃ©servation de l'Information Ordinale
- Ne JAMAIS one-hot des variables ordinales
- Utiliser OrdinalEncoder avec mapping explicite
- Documenter l'ordre des modalitÃ©s

---

## ğŸ“š LIBRAIRIES NÃ‰CESSAIRES

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from category_encoders import TargetEncoder
import pickle
import json
```

---

## ğŸ“Š LIVRABLES ATTENDUS

1. **Classe Python** : `OrdinalCategoricalPreprocessor` avec toutes les mÃ©thodes
2. **Notebook d'exemples** : DÃ©monstration de chaque mÃ©thode
3. **Dataset preprocessÃ©** : Fichier final prÃªt pour modÃ©lisation
4. **Documentation** : Rapport de preprocessing dÃ©taillÃ©
5. **Encoders sauvegardÃ©s** : Fichiers .pkl pour rÃ©utilisation
6. **Tests unitaires** : Validation de chaque mÃ©thode

---

## ğŸ“ BONNES PRATIQUES Ã€ RESPECTER

âœ… **Programmation OrientÃ©e Objet** : CrÃ©er des classes si c'est pertinent.
âœ… **Noms de fonctions explicites** : `encode_ordinal_variables` pas `encode_vars`
âœ… **Docstrings complÃ¨tes** : ParamÃ¨tres, returns, exemples
âœ… **Type hints** : `def func(df: pd.DataFrame) -> pd.DataFrame`
âœ… **Logging** : Logger chaque transformation importante
âœ… **TraÃ§abilitÃ©** : Conserver metadata de chaque transformation
âœ… **Tests** : Valider sur Ã©chantillon avant full dataset
âœ… **ModularitÃ©** : Chaque fonction fait UNE chose
âœ… **RÃ©utilisabilitÃ©** : Code applicable Ã  de nouvelles donnÃ©es

---

## ğŸ“ NOTES FINALES

- Cette TODO list est **exhaustive mais flexible** : adapter selon les donnÃ©es rÃ©elles
- Certaines mÃ©thodes peuvent Ãªtre optionnelles selon les analyses de Phase 1
- Prioriser la **qualitÃ©** sur la vitesse : un bon preprocessing = 80% du succÃ¨s du modÃ¨le
- **Documenter** toutes les dÃ©cisions prises et les justifier

**PrÃªt Ã  commencer le dÃ©veloppement ! ğŸš€**