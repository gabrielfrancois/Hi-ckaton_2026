# üìã TODO LIST - PREPROCESSING VARIABLES ORDINALES ET CAT√âGORIELLES

## üéØ Objectif
Cr√©er une classe `OrdinalPreprocessor`et `CategoricalPreprocessor` avec des m√©thodes ind√©pendantes pour pr√©processer les 96 variables ordinales et 70 variables cat√©gorielles respectivement, en vue de pr√©dire **MathScore** (variable cible √† NE PAS modifier).

---

## üìä CONTEXTE DU DATASET

### Variables √† traiter :
- **96 variables ordinales** (31.2% du dataset)
  - 22 sur le contexte socio-√©conomique familial
  - 13 sur l'environnement de classe
  - 11 sur l'utilisation des TIC
  - 50 autres r√©parties sur 9 domaines

- **70 variables cat√©gorielles** (22.7% du dataset)
  - 28 variables g√©n√©rales (m√©tadonn√©es)
  - 42 variables th√©matiques r√©parties sur 10 domaines

### Variables encod√©es avec haute cardinalit√© :
- **STRATUM** : 1316 strates (urbain/rural, public/priv√©, r√©gions)
- **OCOD** : 620 codes de professions
- **CNT/CNTRYID** : 80 pays (information redondante)

---

## ‚úÖ TODO LIST D√âTAILL√âE

### üîµ PHASE 1 : ANALYSE EXPLORATOIRE (EDA) - Dans Jupyter Notebook
**Objectif** : Explorer les donn√©es r√©elles pour comprendre les distributions et guider le preprocessing

#### [V] 1.1 - Cr√©er notebook `preprocessing/01_eda_ordinal_categorical.ipynb`
- Charger √©chantillon de `data/X_train.csv` (10 000-50 000 lignes)
- Charger dictionnaires de r√©f√©rence du `data/Glossaire.xlsx`

#### [V] 1.2 - Identifier les variables ordinales et cat√©gorielles dans les donn√©es r√©elles
- Utiliser le classification_variable.xlsx pour classifier les variables
- Cr√©er listes: `ordinal_vars` (96 vars) et `categorical_vars` (70 vars)
- V√©rifier coh√©rence avec la structure r√©elle du dataset

#### [V] 1.3 - Analyser distributions des variables ordinales
- Cardinalit√© (nombre de valeurs uniques)
- % de valeurs manquantes par variable
- D√©tecter types d'√©chelles (Likert, fr√©quence, quantit√©)
- Identifier valeurs aberrantes ou codes sp√©ciaux (-99, 97, 98, 99)
- **Visualisations** : histogrammes, boxplots

#### [V] 1.4 - Analyser distributions des variables cat√©gorielles
- Cardinalit√© par variable (faible < 10, moyenne 10-50, haute > 50)
- % de valeurs manquantes
- Identifier cat√©gories rares (< 1% des observations)
- D√©s√©quilibre des classes (imbalance ratio)
- **Focus sp√©cial** : STRATUM (1316), OCOD (620), CNT (80)
- **Visualisations** : barplots, treemaps pour haute cardinalit√©

#### [V] 1.5 - D√©tecter variables redondantes / corr√©l√©es
- Calculer corr√©lations Spearman pour paires de variables ordinales
- Calculer Cram√©r's V pour paires de variables cat√©gorielles
- V√©rifier redondance CNT vs CNTRYID
- **Output** : Liste de variables √† supprimer

#### [V] 1.6 - Analyser patterns de valeurs manquantes
- Matrice de corr√©lation des valeurs manquantes
- Identifier si missing est informatif (MCAR, MAR, MNAR)
- D√©cider strat√©gie d'imputation par variable

#### [V] 1.7 - Documenter conclusions EDA
- Cr√©er rapport markdown avec d√©cisions de preprocessing
- Lister variables √† supprimer, √† regrouper, √† encoder
- D√©finir strat√©gies d'imputation par type de variable
- **Output** : `preprocessing/eda_conclusions.md`

#### [V] 1.8 - Analyser les colonnes d'apr√®s leurs noms et d√©duire ce qu'il faut supprimer / fusionner
- **Output** : `preprocessing/analysis/categorical_variables`, `preprocessing/analysis/ordinal_variables`

#### [V] 1.9 - Analyser des papiers de recherche sur ce sujet, en d√©duire les m√©thodes recommand√©es

#### [] 1.10 - Faire une synth√®se des recommandations et mettre √† jour cette to do list
- Synth√®se √† partir de `preprocessing/analysis/categorial_variables/RESUME_EXECUTIF_Analyse_Categorielles.md`, 
`preprocessing/analysis/ordinal_variables/RESUME_EXECUTIF_Analyse_Ordinales.md`, 
`preprocessing/analysis/analysis_from_pappers.md`et `preprocessing/01_eda_ordinal_categorical.ipynb`
- Mettre √† jour la suite de cette To Do List.

---

### üü° PHASE 2 : CR√âATION DES CLASSES DE PREPROCESSING (dans `classes/`)
**Objectif** : Impl√©menter les classes bas√©es sur les conclusions de l'EDA

#### [] 2.1 - Cr√©er `classes/ordinal_preprocessor.py`
- Classe `OrdinalPreprocessor` avec m√©thodes pour les 96 variables ordinales
- M√©thodes bas√©es sur les conclusions de l'EDA (Phase 1)

#### [] 2.2 - Cr√©er `classes/categorical_preprocessor.py`
- Classe `CategoricalPreprocessor` avec m√©thodes pour les 70 variables cat√©gorielles
- M√©thodes bas√©es sur les conclusions de l'EDA (Phase 1)

---

### üü¢ PHASE 3 : GESTION DES VALEURS MANQUANTES (√† impl√©menter dans les classes)

Strat√©gie √† d√©terminer d'apr√®s la phase 1.

---

### üü† PHASE 4 : TRAITEMENT DES CAT√âGORIES RARES (√† impl√©menter dans les classes)

#### ‚òê 4.1 - `group_rare_categories(df: pd.DataFrame, var: str, threshold: float = 0.01) -> pd.DataFrame`
**Objectif** : Regrouper cat√©gories rares en "Other"
- Pour une variable cat√©gorielle
- Regrouper modalit√©s repr√©sentant < 1% (ou seuil) en "Other"
- Conserver mapping pour interpr√©tabilit√©
- **Output** : DataFrame avec cat√©gories regroup√©es

#### ‚òê 4.2 - `reduce_stratum_dimensionality(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : R√©duire les 1316 strates en features exploitables
- Parser STRATUM pour extraire :
  - `stratum_location` : Urban / Rural
  - `stratum_region` : North / Center / South / etc.
  - `stratum_type` : Public / Private
  - `stratum_country` : Code pays (3 lettres)
- Supprimer STRATUM original
- **Output** : DataFrame avec 4 nouvelles variables + suppression STRATUM

#### ‚òê 4.3 - `group_occupations_by_major_group(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Regrouper les 620 professions en grands groupes ISCO
- Utiliser le 1er chiffre du code OCOD pour cr√©er 10 groupes :
  - 0: Armed forces
  - 1: Managers
  - 2: Professionals
  - 3: Technicians
  - 4: Clerical support
  - 5: Service and sales
  - 6: Skilled agricultural
  - 7: Craft workers
  - 8: Plant operators
  - 9: Elementary occupations
- **Output** : DataFrame avec OCOD remplac√© par OCOD_major_group

#### ‚òê 4.4 - `resolve_cnt_cntryid_redundancy(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Supprimer la redondance entre CNT et CNTRYID
- V√©rifier corr√©lation parfaite
- Garder CNT (plus lisible : codes 3 lettres)
- Supprimer CNTRYID
- **Output** : DataFrame sans CNTRYID

---

### üîµ PHASE 5 : ENCODAGE DES VARIABLES (√† impl√©menter dans les classes)

#### ‚òê 5.1 - `encode_ordinal_variables(df: pd.DataFrame, mapping_dict: dict = None) -> pd.DataFrame`
**Objectif** : Encoder les variables ordinales en pr√©servant l'ordre
- Utiliser OrdinalEncoder de sklearn
- Cr√©er mappings explicites pour √©chelles Likert, fr√©quences
- Exemple : {"Never": 0, "Rarely": 1, "Sometimes": 2, "Often": 3, "Always": 4}
- Stocker encoders dans `self.encoders`
- **Output** : DataFrame avec ordinales encod√©es en int

#### ‚òê 5.2 - `encode_binary_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Encoder variables cat√©gorielles binaires
- Pour variables avec exactement 2 modalit√©s (ex: Gender, OECD Yes/No)
- Encoder en 0/1 avec LabelEncoder
- **Output** : DataFrame avec binaires encod√©es

#### ‚òê 5.3 - `onehot_encode_low_cardinality(df: pd.DataFrame, max_categories: int = 10) -> pd.DataFrame`
**Objectif** : One-Hot Encoding pour variables √† faible cardinalit√©
- Pour variables cat√©gorielles avec ‚â§ 10 modalit√©s
- Utiliser pd.get_dummies ou OneHotEncoder
- Nommer colonnes : `var_name_category`
- **Output** : DataFrame avec colonnes one-hot cr√©√©es

#### ‚òê 5.4 - `target_encode_high_cardinality(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Target Encoding pour variables √† haute cardinalit√©
- Pour variables avec > 10 modalit√©s (CNT, langues, etc.)
- Encoder par moyenne de MathScore pour chaque cat√©gorie
- Ajouter r√©gularisation (smoothing) pour cat√©gories rares
- Attention au data leakage : utiliser cross-validation
- **Output** : DataFrame avec target encoding appliqu√©

#### ‚òê 5.5 - `frequency_encode_categorical(df: pd.DataFrame, vars_list: list) -> pd.DataFrame`
**Objectif** : Frequency Encoding (alternative au Target Encoding)
- Encoder par fr√©quence d'apparition de chaque cat√©gorie
- Moins risqu√© que target encoding (pas de leakage)
- **Output** : DataFrame avec frequency encoding appliqu√©

---

### üî¥ PHASE 6 : VALIDATION ET CONTR√îLE QUALIT√â (√† impl√©menter dans les classes)

#### ‚òê 6.1 - `validate_no_missing_after_preprocessing(df: pd.DataFrame) -> bool`
**Objectif** : V√©rifier qu'il n'y a plus de valeurs manquantes
- Compter les NaN restants
- Lever une exception si NaN d√©tect√©s
- **Output** : True si OK, raise ValueError sinon

#### ‚òê 6.2 - `validate_dtypes_after_encoding(df: pd.DataFrame) -> pd.DataFrame`
**Objectif** : V√©rifier les types de donn√©es apr√®s encodage
- Ordinales encod√©es ‚Üí int ou float
- Cat√©gorielles encod√©es ‚Üí int ou float
- Pas de type 'object' sauf si voulu
- **Output** : DataFrame de validation avec [column, expected_dtype, actual_dtype, status]

#### ‚òê 6.3 - `check_target_variable_unchanged(df_before: pd.DataFrame, df_after: pd.DataFrame) -> bool`
**Objectif** : V√©rifier que MathScore n'a pas √©t√© modifi√©
- Comparer MathScore avant et apr√®s preprocessing
- Lever exception si diff√©rences d√©tect√©es
- **Output** : True si identique, raise ValueError sinon

#### ‚òê 6.4 - `generate_preprocessing_report(df_before: pd.DataFrame, df_after: pd.DataFrame) -> dict`
**Objectif** : G√©n√©rer un rapport de preprocessing
- Nombre de variables avant/apr√®s
- Variables supprim√©es et raison
- Variables cr√©√©es (one-hot, indicatrices missing)
- Statistiques d'encodage
- **Output** : Dict avec toutes les m√©tadonn√©es

#### ‚òê 6.5 - `detect_data_leakage_risk(df: pd.DataFrame) -> list`
**Objectif** : D√©tecter les risques de data leakage
- Identifier si target encoding fait sans CV
- Identifier si imputation utilise statistiques globales
- Identifier si normalisation faite sur tout le dataset
- **Output** : Liste des warnings de leakage potentiel

---

### üü£ PHASE 7 : PIPELINE ET SAUVEGARDE (√† impl√©menter dans les classes)

#### ‚òê 7.1 - `create_preprocessing_pipeline(steps: list) -> Pipeline`
**Objectif** : Cr√©er un pipeline sklearn r√©utilisable
- Encha√Æner les transformations dans l'ordre
- Utiliser ColumnTransformer pour appliquer transformations par type
- **Output** : Pipeline sklearn fitted

#### ‚òê 7.2 - `save_encoders_and_mappings(filepath: str) -> None`
**Objectif** : Sauvegarder les encoders pour r√©utilisation
- Pickler les OrdinalEncoder, LabelEncoder, OneHotEncoder
- Sauvegarder les mappings de r√©f√©rence
- Sauvegarder les listes de variables par type
- **Output** : Fichier .pkl

#### ‚òê 7.3 - `export_preprocessed_data(df: pd.DataFrame, filepath: str) -> None`
**Objectif** : Exporter le dataset pr√©process√©
- Sauvegarder en CSV ou Parquet
- Inclure m√©tadonn√©es dans un fichier s√©par√©
- **Output** : Fichiers data + metadata

#### ‚òê 7.4 - `transform_new_data(df_new: pd.DataFrame) -> pd.DataFrame`
**Objectif** : Appliquer le preprocessing √† de nouvelles donn√©es
- Charger les encoders sauvegard√©s
- Appliquer les m√™mes transformations
- G√©rer les nouvelles cat√©gories inconnues
- **Output** : DataFrame transform√©


---

## ‚ö†Ô∏è POINTS D'ATTENTION CRITIQUES

### üö® Data Leakage
- **Target encoding** : OBLIGATOIRE d'utiliser cross-validation
- **Imputation** : Calculer statistiques UNIQUEMENT sur train set
- **Scaling** : Fit sur train, transform sur test

### üö® Gestion de MathScore (Cible)
- **NE JAMAIS** modifier, imputer, ou encoder MathScore
- V√©rifier apr√®s chaque transformation avec `check_target_variable_unchanged()`
- Exclure MathScore de toutes les transformations

### üö® Haute Cardinalit√©
- STRATUM (1316) et OCOD (620) : **R√âDUCTION OBLIGATOIRE**
- Ne JAMAIS faire de one-hot sur ces variables
- Privil√©gier feature engineering intelligent

### üö® Variables Redondantes
- CNT vs CNTRYID : supprimer l'un des deux
- V√©rifier corr√©lations avant encodage

### üö® Pr√©servation de l'Information Ordinale
- Ne JAMAIS one-hot des variables ordinales
- Utiliser OrdinalEncoder avec mapping explicite
- Documenter l'ordre des modalit√©s

---

## üìä LIVRABLES ATTENDUS

1. **Classe Python** : `OrdinalCategoricalPreprocessor` avec toutes les m√©thodes
2. **Notebook d'exemples** : D√©monstration de chaque m√©thode
3. **Dataset preprocess√©** : Fichier final pr√™t pour mod√©lisation
4. **Documentation** : Rapport de preprocessing d√©taill√©
5. **Encoders sauvegard√©s** : Fichiers .pkl pour r√©utilisation

---

## üéì BONNES PRATIQUES √Ä RESPECTER

‚úÖ **Programmation Orient√©e Objet** : Cr√©er des classes si c'est pertinent.
‚úÖ **Noms de fonctions explicites** : `encode_ordinal_variables` pas `encode_vars`
‚úÖ **Docstrings compl√®tes** : Param√®tres, returns, exemples
‚úÖ **Type hints** : `def func(df: pd.DataFrame) -> pd.DataFrame`
‚úÖ **Logging** : Logger chaque transformation importante
‚úÖ **Tra√ßabilit√©** : Conserver metadata de chaque transformation
‚úÖ **Tests** : Valider sur √©chantillon avant full dataset
‚úÖ **Modularit√©** : Chaque fonction fait UNE chose
‚úÖ **R√©utilisabilit√©** : Code applicable √† de nouvelles donn√©es

---

## üìù NOTES FINALES

- Cette TODO list est **exhaustive mais flexible** : adapter selon les donn√©es r√©elles
- Certaines m√©thodes peuvent √™tre optionnelles selon les analyses de Phase 1
- Prioriser la **vitesse** sur la qualit√© : seulement quelques heures pour cet exercice!
- **Documenter** toutes les d√©cisions prises et les justifier