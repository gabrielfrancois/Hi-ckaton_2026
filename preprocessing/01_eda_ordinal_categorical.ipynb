{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Variables Ordinales et CatÃ©gorielles\n",
    "\n",
    "**Objectif** : Analyser les 96 variables ordinales et 70 variables catÃ©gorielles du dataset PISA pour guider le preprocessing.\n",
    "\n",
    "**DonnÃ©es** :\n",
    "- Dataset : `data/X_train.csv`\n",
    "- Classification : `data/classification_variables.xlsx`\n",
    "- Glossaire : `data/Glossaire.xlsx`\n",
    "- Documentation : `preprocessing/glossaire_analysis.md`\n",
    "\n",
    "**Note** : On utilise **Polars** pour des performances optimales sur les gros datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports rÃ©ussis\n",
      "Polars version: 1.35.2\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "\n",
    "# Pour lire les fichiers Excel (Polars ne supporte pas directement Excel)\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Imports rÃ©ussis\")\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des donnÃ©es\n",
    "\n",
    "### 2.1 Charger Ã©chantillon de X_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de 50000 lignes de X_train.csv avec Polars...\n",
      "\n",
      "âœ… Dataset chargÃ© : 50,000 lignes, 307 colonnes\n",
      "MÃ©moire estimÃ©e : 102.50 MB\n"
     ]
    }
   ],
   "source": [
    "# Charger un Ã©chantillon avec Polars (beaucoup plus rapide que pandas)\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "print(f\"Chargement de {SAMPLE_SIZE} lignes de X_train.csv avec Polars...\")\n",
    "df = pl.read_csv('../data/X_train.csv', n_rows=SAMPLE_SIZE)\n",
    "\n",
    "print(f\"\\nâœ… Dataset chargÃ© : {df.shape[0]:,} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"MÃ©moire estimÃ©e : {df.estimated_size('mb'):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 307)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>Year</th><th>CNT</th><th>CNTRYID</th><th>CNTSCHID</th><th>CNTSTUID</th><th>CYC</th><th>NatCen</th><th>STRATUM</th><th>SUBNATIO</th><th>OECD</th><th>ADMINMODE</th><th>LANGTEST_QQQ</th><th>LANGTEST_COG</th><th>LANGTEST_PAQ</th><th>Option_CT</th><th>Option_FL</th><th>Option_ICTQ</th><th>Option_WBQ</th><th>Option_PQ</th><th>Option_TQ</th><th>Option_UH</th><th>ST001D01T</th><th>ST003D02T</th><th>ST003D03T</th><th>ST004D01T</th><th>EFFORT1</th><th>EFFORT2</th><th>OCOD1</th><th>OCOD2</th><th>OCOD3</th><th>AGE</th><th>GRADE</th><th>ISCEDP</th><th>IMMIG</th><th>COBN_S</th><th>MISSSC</th><th>&hellip;</th><th>math_q4_total_timing</th><th>math_q5_total_timing</th><th>math_q6_total_timing</th><th>math_q7_total_timing</th><th>math_q8_total_timing</th><th>math_q9_total_timing</th><th>math_q10_total_timing</th><th>math_q11_total_timing</th><th>math_q12_total_timing</th><th>math_q13_total_timing</th><th>math_q14_total_timing</th><th>math_q15_total_timing</th><th>math_q16_total_timing</th><th>math_q17_total_timing</th><th>math_q18_total_timing</th><th>math_q19_total_timing</th><th>math_q20_total_timing</th><th>math_q21_total_timing</th><th>science_q1_total_timing</th><th>science_q2_total_timing</th><th>science_q3_total_timing</th><th>science_q4_total_timing</th><th>science_q5_total_timing</th><th>science_q6_total_timing</th><th>science_q7_total_timing</th><th>science_q8_total_timing</th><th>science_q9_total_timing</th><th>science_q10_total_timing</th><th>science_q11_total_timing</th><th>science_q12_total_timing</th><th>science_q13_total_timing</th><th>science_q14_total_timing</th><th>science_q15_total_timing</th><th>science_q16_total_timing</th><th>science_q17_total_timing</th><th>science_q18_total_timing</th><th>science_q19_total_timing</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>384002</td><td>2022</td><td>&quot;NLD&quot;</td><td>528.0</td><td>5.2800132e7</td><td>5.2801144e7</td><td>&quot;08MS&quot;</td><td>52800</td><td>&quot;NLD06&quot;</td><td>5280000</td><td>1.0</td><td>2.0</td><td>322.0</td><td>322.0</td><td>null</td><td>1.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>10.0</td><td>2.0</td><td>2007.0</td><td>1.0</td><td>8.0</td><td>8.0</td><td>3256.0</td><td>2212.0</td><td>9999.0</td><td>15.75</td><td>0.0</td><td>244.0</td><td>2.0</td><td>52800.0</td><td>0.0</td><td>&hellip;</td><td>110641.333333</td><td>96368.333333</td><td>71813.5</td><td>98875.0</td><td>63689.4</td><td>37232.0</td><td>88274.666667</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>84299.333333</td><td>90156.0</td><td>66612.0</td><td>73547.6</td><td>68912.0</td><td>56623.25</td><td>104190.5</td><td>132623.25</td><td>105785.666667</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1118072</td><td>2018</td><td>&quot;QAZ&quot;</td><td>31.0</td><td>3.100106e6</td><td>3.100424e6</td><td>&quot;07MS&quot;</td><td>3100</td><td>&quot;QAZ0101&quot;</td><td>310000</td><td>0.0</td><td>2.0</td><td>803.0</td><td>803.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10.0</td><td>11.0</td><td>2002.0</td><td>2.0</td><td>7.0</td><td>6.0</td><td>2330.0</td><td>9313.0</td><td>0.0</td><td>15.42</td><td>0.0</td><td>null</td><td>null</td><td>3100.0</td><td>null</td><td>&hellip;</td><td>116718.0</td><td>61003.0</td><td>34117.0</td><td>56901.667969</td><td>76615.0</td><td>21500.333984</td><td>27087.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>845454</td><td>2018</td><td>&quot;FRA&quot;</td><td>250.0</td><td>2.500001e7</td><td>2.5005207e7</td><td>&quot;07MS&quot;</td><td>25000</td><td>&quot;FRA0101&quot;</td><td>2500000</td><td>1.0</td><td>2.0</td><td>493.0</td><td>493.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10.0</td><td>5.0</td><td>2002.0</td><td>1.0</td><td>null</td><td>null</td><td>3323.0</td><td>3322.0</td><td>7512.0</td><td>16.0</td><td>0.0</td><td>null</td><td>null</td><td>25000.0</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>81443.664062</td><td>190243.40625</td><td>36257.0</td><td>148073.5</td><td>2121.249756</td><td>90540.335938</td><td>57954.0</td><td>84633.0</td><td>45332.0</td><td>87686.5</td><td>13164.75</td><td>1187.199951</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1728613</td><td>2015</td><td>&quot;QES&quot;</td><td>971.0</td><td>9.710024e7</td><td>9.7127584e7</td><td>&quot;06MS&quot;</td><td>72400</td><td>&quot;ESP1633&quot;</td><td>7241600</td><td>2.0</td><td>2.0</td><td>156.0</td><td>156.0</td><td>null</td><td>null</td><td>1.0</td><td>1.0</td><td>null</td><td>1.0</td><td>1.0</td><td>0.0</td><td>10.0</td><td>3.0</td><td>1999.0</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>16.17</td><td>null</td><td>null</td><td>1.0</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>62224.667969</td><td>83289.664062</td><td>91510.75</td><td>100649.5</td><td>82378.5</td><td>74348.25</td><td>91230.664062</td><td>49789.0</td><td>37855.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1083243</td><td>2018</td><td>&quot;PHL&quot;</td><td>608.0</td><td>6.0800071e7</td><td>6.0802698e7</td><td>&quot;07MS&quot;</td><td>60800</td><td>&quot;PHL0011&quot;</td><td>6080000</td><td>0.0</td><td>2.0</td><td>313.0</td><td>313.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>8.0</td><td>9.0</td><td>2002.0</td><td>1.0</td><td>10.0</td><td>10.0</td><td>9701.0</td><td>7132.0</td><td>2161.0</td><td>15.5</td><td>-1.0</td><td>null</td><td>null</td><td>60800.0</td><td>null</td><td>&hellip;</td><td>82282.5</td><td>121010.5</td><td>191303.0</td><td>77024.335938</td><td>197619.671875</td><td>64659.667969</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 307)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚         â”† Year â”† CNT â”† CNTRYID â”† â€¦ â”† science_q16_t â”† science_q17_t â”† science_q18_ â”† science_q19_ â”‚\n",
       "â”‚ ---     â”† ---  â”† --- â”† ---     â”†   â”† otal_timing   â”† otal_timing   â”† total_timing â”† total_timing â”‚\n",
       "â”‚ i64     â”† i64  â”† str â”† f64     â”†   â”† ---           â”† ---           â”† ---          â”† ---          â”‚\n",
       "â”‚         â”†      â”†     â”†         â”†   â”† str           â”† str           â”† str          â”† str          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 384002  â”† 2022 â”† NLD â”† 528.0   â”† â€¦ â”† null          â”† null          â”† null         â”† null         â”‚\n",
       "â”‚ 1118072 â”† 2018 â”† QAZ â”† 31.0    â”† â€¦ â”† null          â”† null          â”† null         â”† null         â”‚\n",
       "â”‚ 845454  â”† 2018 â”† FRA â”† 250.0   â”† â€¦ â”† null          â”† null          â”† null         â”† null         â”‚\n",
       "â”‚ 1728613 â”† 2015 â”† QES â”† 971.0   â”† â€¦ â”† null          â”† null          â”† null         â”† null         â”‚\n",
       "â”‚ 1083243 â”† 2018 â”† PHL â”† 608.0   â”† â€¦ â”† null          â”† null          â”† null         â”† null         â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AperÃ§u des donnÃ©es\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema du dataset :\n",
      "Schema({'': Int64, 'Year': Int64, 'CNT': String, 'CNTRYID': Float64, 'CNTSCHID': Float64, 'CNTSTUID': Float64, 'CYC': String, 'NatCen': Int64, 'STRATUM': String, 'SUBNATIO': Int64, 'OECD': Float64, 'ADMINMODE': Float64, 'LANGTEST_QQQ': Float64, 'LANGTEST_COG': Float64, 'LANGTEST_PAQ': Float64, 'Option_CT': Float64, 'Option_FL': Float64, 'Option_ICTQ': Float64, 'Option_WBQ': Float64, 'Option_PQ': Float64, 'Option_TQ': Float64, 'Option_UH': Float64, 'ST001D01T': Float64, 'ST003D02T': Float64, 'ST003D03T': Float64, 'ST004D01T': Float64, 'EFFORT1': Float64, 'EFFORT2': Float64, 'OCOD1': Float64, 'OCOD2': Float64, 'OCOD3': Float64, 'AGE': Float64, 'GRADE': Float64, 'ISCEDP': Float64, 'IMMIG': Float64, 'COBN_S': Float64, 'MISSSC': Float64, 'MATHEASE': Float64, 'WB153': Float64, 'ST253': Float64, 'ST311': Float64, 'WB165': Float64, 'IC171': Float64, 'ST038': Float64, 'IC180': Float64, 'ST255': Float64, 'ST350': Float64, 'ST034': Float64, 'FL162': Float64, 'PA195': Float64, 'ST330': Float64, 'FL169': Float64, 'ST230': Float64, 'ST331': Float64, 'ST354': Float64, 'PA042': Float64, 'ST305': Float64, 'ST258': Float64, 'PA167': Float64, 'IC176': Float64, 'ST352': Float64, 'IC173': Float64, 'PA188': Float64, 'ST283': Float64, 'ST256': Float64, 'WB154': Float64, 'WB166': Float64, 'FL150': Float64, 'ST266': Float64, 'IC174': Float64, 'ST005': Float64, 'ST021': Float64, 'IC182': Float64, 'FL164': Float64, 'FL160': Float64, 'ST267': Float64, 'ST338': Float64, 'ST351': Float64, 'ST342': Float64, 'ST355': Float64, 'FL166': Float64, 'PA183': Float64, 'ST007': Float64, 'FL170': Float64, 'WB168': Float64, 'WB163': Float64, 'IC184': Float64, 'ST348': Float64, 'ST226': Float64, 'ST336': Float64, 'WB160': Float64, 'FL167': Float64, 'PA166': Float64, 'ST250': Float64, 'ST268': Float64, 'IC177': Float64, 'PA196': Float64, 'IC183': Float64, 'ST275': Float64, 'IC172': Float64, 'IC175': Float64, 'ST340': Float64, 'ST296': Float64, 'ST270': Float64, 'ST289': Float64, 'ST353': Float64, 'ST349': Float64, 'ST301': Float64, 'PA186': Float64, 'WB164': Float64, 'PA197': Float64, 'ST290': Float64, 'ST062': Float64, 'ST254': Float64, 'ST345': Float64, 'ST322': Float64, 'IC170': Float64, 'PA006': Float64, 'ST347': Float64, 'ST343': Float64, 'ST307': Float64, 'ST160': Float64, 'ST168': Float64, 'ST161': Float64, 'ST153': Float64, 'ST188': Float64, 'PA009': Float64, 'PA182': Float64, 'PA004': Float64, 'ST059': Float64, 'ST150': Float64, 'PA158': Float64, 'WB155': Float64, 'WB162': Float64, 'ST127': Float64, 'EC162': Float64, 'ST104': Float64, 'PA003': Float64, 'ST125': Float64, 'ST163': Float64, 'ST297': Float64, 'ST006': Float64, 'ST263': Float64, 'PA162': Float64, 'ST164': Float64, 'ST008': Float64, 'ST152': Float64, 'EC031': Float64, 'WB177': Float64, 'ST183': Float64, 'EC012': Float64, 'ST223': Float64, 'ST177': Float64, 'ST102': Float64, 'PA041': Float64, 'EC163': Float64, 'PA008': Float64, 'PA177': Float64, 'ST175': Float64, 'ST273': Float64, 'EC153': Float64, 'ST212': Float64, 'PA154': Float64, 'PA156': Float64, 'PA007': Float64, 'PA159': Float64, 'ST300': Float64, 'ST036': Float64, 'PA018': Float64, 'ST208': Float64, 'ST165': Float64, 'PA175': Float64, 'ST167': Float64, 'ST211': Float64, 'WB176': Float64, 'ST158': Float64, 'WB178': Float64, 'IC014': Float64, 'ST213': Float64, 'ST327': Float64, 'ST251': Float64, 'ST260': Float64, 'PA160': Float64, 'ST095': Float64, 'PA033': Float64, 'ST146': Float64, 'ST098': Float64, 'ST100': Float64, 'ST097': Float64, 'ST113': Float64, 'PA002': Float64, 'PA032': Float64, 'IC001': Float64, 'ST008_duplicated_0': Float64, 'ST006_duplicated_0': Float64, 'PA008_duplicated_0': Float64, 'ST011': Float64, 'reading_q1_average_score': Float64, 'reading_q2_average_score': Float64, 'reading_q3_average_score': Float64, 'reading_q4_average_score': Float64, 'reading_q5_average_score': Float64, 'reading_q6_average_score': Float64, 'reading_q7_average_score': Float64, 'reading_q8_average_score': Float64, 'reading_q9_average_score': Float64, 'reading_q10_average_score': String, 'reading_q11_average_score': String, 'reading_q12_average_score': String, 'reading_q13_average_score': String, 'reading_q14_average_score': String, 'reading_q15_average_score': String, 'math_q1_average_score': Float64, 'math_q2_average_score': Float64, 'math_q3_average_score': Float64, 'math_q4_average_score': Float64, 'math_q5_average_score': Float64, 'math_q6_average_score': Float64, 'math_q7_average_score': Float64, 'math_q8_average_score': Float64, 'math_q9_average_score': Float64, 'math_q10_average_score': Float64, 'math_q11_average_score': Float64, 'math_q12_average_score': Float64, 'math_q13_average_score': Float64, 'math_q14_average_score': String, 'math_q15_average_score': String, 'math_q16_average_score': String, 'math_q17_average_score': String, 'math_q18_average_score': String, 'math_q19_average_score': String, 'math_q20_average_score': String, 'math_q21_average_score': String, 'science_q1_average_score': Float64, 'science_q2_average_score': Float64, 'science_q3_average_score': Float64, 'science_q4_average_score': Float64, 'science_q5_average_score': Float64, 'science_q6_average_score': Float64, 'science_q7_average_score': Float64, 'science_q8_average_score': Float64, 'science_q9_average_score': Float64, 'science_q10_average_score': Float64, 'science_q11_average_score': Float64, 'science_q12_average_score': Float64, 'science_q13_average_score': String, 'science_q14_average_score': String, 'science_q15_average_score': String, 'science_q16_average_score': String, 'science_q17_average_score': String, 'science_q18_average_score': String, 'science_q19_average_score': String, 'reading_q1_total_timing': Float64, 'reading_q2_total_timing': Float64, 'reading_q3_total_timing': Float64, 'reading_q4_total_timing': Float64, 'reading_q5_total_timing': Float64, 'reading_q6_total_timing': Float64, 'reading_q7_total_timing': Float64, 'reading_q8_total_timing': Float64, 'reading_q9_total_timing': Float64, 'reading_q10_total_timing': String, 'reading_q11_total_timing': String, 'reading_q12_total_timing': String, 'reading_q13_total_timing': String, 'reading_q14_total_timing': String, 'reading_q15_total_timing': String, 'math_q1_total_timing': Float64, 'math_q2_total_timing': Float64, 'math_q3_total_timing': Float64, 'math_q4_total_timing': Float64, 'math_q5_total_timing': Float64, 'math_q6_total_timing': Float64, 'math_q7_total_timing': Float64, 'math_q8_total_timing': Float64, 'math_q9_total_timing': Float64, 'math_q10_total_timing': Float64, 'math_q11_total_timing': Float64, 'math_q12_total_timing': Float64, 'math_q13_total_timing': Float64, 'math_q14_total_timing': Float64, 'math_q15_total_timing': String, 'math_q16_total_timing': String, 'math_q17_total_timing': String, 'math_q18_total_timing': String, 'math_q19_total_timing': String, 'math_q20_total_timing': String, 'math_q21_total_timing': String, 'science_q1_total_timing': Float64, 'science_q2_total_timing': Float64, 'science_q3_total_timing': Float64, 'science_q4_total_timing': Float64, 'science_q5_total_timing': Float64, 'science_q6_total_timing': Float64, 'science_q7_total_timing': Float64, 'science_q8_total_timing': Float64, 'science_q9_total_timing': Float64, 'science_q10_total_timing': Float64, 'science_q11_total_timing': Float64, 'science_q12_total_timing': Float64, 'science_q13_total_timing': String, 'science_q14_total_timing': String, 'science_q15_total_timing': String, 'science_q16_total_timing': String, 'science_q17_total_timing': String, 'science_q18_total_timing': String, 'science_q19_total_timing': String})\n",
      "\n",
      "Nombre total de colonnes : 307\n",
      "\n",
      "PremiÃ¨res colonnes : ['', 'Year', 'CNT', 'CNTRYID', 'CNTSCHID', 'CNTSTUID', 'CYC', 'NatCen', 'STRATUM', 'SUBNATIO', 'OECD', 'ADMINMODE', 'LANGTEST_QQQ', 'LANGTEST_COG', 'LANGTEST_PAQ', 'Option_CT', 'Option_FL', 'Option_ICTQ', 'Option_WBQ', 'Option_PQ']\n"
     ]
    }
   ],
   "source": [
    "# Info gÃ©nÃ©rale (schema = types de colonnes en Polars)\n",
    "print(\"Schema du dataset :\")\n",
    "print(df.schema)\n",
    "\n",
    "print(f\"\\nNombre total de colonnes : {len(df.columns)}\")\n",
    "print(f\"\\nPremiÃ¨res colonnes : {df.columns[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Charger les dictionnaires de rÃ©fÃ©rence du Glossaire\n",
    "\n",
    "Note: Pour lire Excel, on utilise pandas puis on convertit en Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des feuilles du Glossaire...\n",
      "Feuilles disponibles : ['General', 'CNT', 'CNTRYID', 'STRATUM', 'ISCEDP', 'OCOD']\n",
      "\n",
      "âœ… Glossaire General : (308, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>question_code</th><th>description</th><th>thematic domain</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Year&quot;</td><td>&quot;Year of the test&quot;</td><td>&quot;General&quot;</td></tr><tr><td>&quot;CNT&quot;</td><td>&quot;Country code 3-character&quot;</td><td>&quot;General&quot;</td></tr><tr><td>&quot;CNTRYID&quot;</td><td>&quot;Country Identifier&quot;</td><td>&quot;General&quot;</td></tr><tr><td>&quot;CNTSCHID&quot;</td><td>&quot;International School ID&quot;</td><td>&quot;General&quot;</td></tr><tr><td>&quot;CNTSTUID&quot;</td><td>&quot;International Student ID&quot;</td><td>&quot;General&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ question_code â”† description              â”† thematic domain â”‚\n",
       "â”‚ ---           â”† ---                      â”† ---             â”‚\n",
       "â”‚ str           â”† str                      â”† str             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Year          â”† Year of the test         â”† General         â”‚\n",
       "â”‚ CNT           â”† Country code 3-character â”† General         â”‚\n",
       "â”‚ CNTRYID       â”† Country Identifier       â”† General         â”‚\n",
       "â”‚ CNTSCHID      â”† International School ID  â”† General         â”‚\n",
       "â”‚ CNTSTUID      â”† International Student ID â”† General         â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger toutes les feuilles du Glossaire (avec pandas car Excel)\n",
    "glossaire_path = '../data/Glossaire.xlsx'\n",
    "\n",
    "print(\"Chargement des feuilles du Glossaire...\")\n",
    "glossaire_sheets = pd.ExcelFile(glossaire_path).sheet_names\n",
    "print(f\"Feuilles disponibles : {glossaire_sheets}\")\n",
    "\n",
    "# Charger la feuille principale (General) et convertir en Polars\n",
    "glossaire_general_pd = pd.read_excel(glossaire_path, sheet_name='General')\n",
    "glossaire_general = pl.from_pandas(glossaire_general_pd)\n",
    "\n",
    "print(f\"\\nâœ… Glossaire General : {glossaire_general.shape}\")\n",
    "glossaire_general.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT: 80 entrÃ©es\n",
      "CNTRYID: 80 entrÃ©es\n",
      "STRATUM: 1316 entrÃ©es\n",
      "ISCEDP: 59 entrÃ©es\n",
      "OCOD: 620 entrÃ©es\n",
      "\n",
      "âœ… Dictionnaires chargÃ©s : ['CNT', 'CNTRYID', 'STRATUM', 'ISCEDP', 'OCOD']\n"
     ]
    }
   ],
   "source": [
    "# Charger les dictionnaires de rÃ©fÃ©rence\n",
    "reference_dicts = {}\n",
    "\n",
    "for sheet in ['CNT', 'CNTRYID', 'STRATUM', 'ISCEDP', 'OCOD']:\n",
    "    if sheet in glossaire_sheets:\n",
    "        # Charger avec pandas puis convertir en Polars\n",
    "        temp_pd = pd.read_excel(glossaire_path, sheet_name=sheet)\n",
    "        reference_dicts[sheet] = pl.from_pandas(temp_pd)\n",
    "        print(f\"{sheet}: {reference_dicts[sheet].shape[0]} entrÃ©es\")\n",
    "\n",
    "print(f\"\\nâœ… Dictionnaires chargÃ©s : {list(reference_dicts.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identification des variables ordinales et catÃ©gorielles\n",
    "\n",
    "La classification est donnÃ©e dans `data/classification_variables.xlsx`\n",
    "\n",
    "Selon `glossaire_analysis.md` :\n",
    "- 96 variables ordinales (31.2%)\n",
    "- 70 variables catÃ©gorielles (22.7%)\n",
    "- 135 variables numÃ©riques (43.8%)\n",
    "- 7 variables de groupement (2.3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de la classification des variables...\n",
      "\n",
      "âœ… Classification chargÃ©e : (135, 4)\n",
      "\n",
      "Colonnes disponibles : ['Variables NumÃ©riques', 'Variables Ordinales', 'Variables CatÃ©gorielles', 'Variables de Groupement']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Variables NumÃ©riques</th><th>Variables Ordinales</th><th>Variables CatÃ©gorielles</th><th>Variables de Groupement</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;AGE&quot;</td><td>&quot;ISCEDP&quot;</td><td>&quot;CYC&quot;</td><td>&quot;Year&quot;</td></tr><tr><td>&quot;COBN_S&quot;</td><td>&quot;ST253&quot;</td><td>&quot;NatCen&quot;</td><td>&quot;CNT&quot;</td></tr><tr><td>&quot;WB165&quot;</td><td>&quot;ST311&quot;</td><td>&quot;SUBNATIO&quot;</td><td>&quot;CNTRYID&quot;</td></tr><tr><td>&quot;FL169&quot;</td><td>&quot;IC171&quot;</td><td>&quot;OECD&quot;</td><td>&quot;CNTSCHID&quot;</td></tr><tr><td>&quot;WB166&quot;</td><td>&quot;ST038&quot;</td><td>&quot;ADMINMODE&quot;</td><td>&quot;CNTSTUID&quot;</td></tr><tr><td>&quot;FL164&quot;</td><td>&quot;IC180&quot;</td><td>&quot;LANGTEST_QQQ&quot;</td><td>&quot;STRATUM&quot;</td></tr><tr><td>&quot;ST355&quot;</td><td>&quot;ST255&quot;</td><td>&quot;LANGTEST_COG&quot;</td><td>&quot;BOOKID&quot;</td></tr><tr><td>&quot;IC177&quot;</td><td>&quot;ST034&quot;</td><td>&quot;LANGTEST_PAQ&quot;</td><td>null</td></tr><tr><td>&quot;IC183&quot;</td><td>&quot;PA195&quot;</td><td>&quot;Option_CT&quot;</td><td>null</td></tr><tr><td>&quot;ST345&quot;</td><td>&quot;ST230&quot;</td><td>&quot;Option_FL&quot;</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ Variables NumÃ©riques â”† Variables Ordinales â”† Variables CatÃ©gorielles â”† Variables de Groupement â”‚\n",
       "â”‚ ---                  â”† ---                 â”† ---                     â”† ---                     â”‚\n",
       "â”‚ str                  â”† str                 â”† str                     â”† str                     â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ AGE                  â”† ISCEDP              â”† CYC                     â”† Year                    â”‚\n",
       "â”‚ COBN_S               â”† ST253               â”† NatCen                  â”† CNT                     â”‚\n",
       "â”‚ WB165                â”† ST311               â”† SUBNATIO                â”† CNTRYID                 â”‚\n",
       "â”‚ FL169                â”† IC171               â”† OECD                    â”† CNTSCHID                â”‚\n",
       "â”‚ WB166                â”† ST038               â”† ADMINMODE               â”† CNTSTUID                â”‚\n",
       "â”‚ FL164                â”† IC180               â”† LANGTEST_QQQ            â”† STRATUM                 â”‚\n",
       "â”‚ ST355                â”† ST255               â”† LANGTEST_COG            â”† BOOKID                  â”‚\n",
       "â”‚ IC177                â”† ST034               â”† LANGTEST_PAQ            â”† null                    â”‚\n",
       "â”‚ IC183                â”† PA195               â”† Option_CT               â”† null                    â”‚\n",
       "â”‚ ST345                â”† ST230               â”† Option_FL               â”† null                    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger la classification des variables\n",
    "classification_path = '../data/classification_variables.xlsx'\n",
    "\n",
    "print(\"Chargement de la classification des variables...\")\n",
    "classification_pd = pd.read_excel(classification_path)\n",
    "classification = pl.from_pandas(classification_pd)\n",
    "\n",
    "print(f\"\\nâœ… Classification chargÃ©e : {classification.shape}\")\n",
    "print(f\"\\nColonnes disponibles : {classification.columns}\")\n",
    "classification.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Variables ordinales : 96\n",
      "âœ… Variables catÃ©gorielles : 70\n",
      "âœ… Variables numÃ©riques : 135\n",
      "âœ… Variables de groupement : 7\n",
      "\n",
      "ğŸ“‹ Exemples de variables ordinales : ['ISCEDP', 'ST253', 'ST311', 'IC171', 'ST038', 'IC180', 'ST255', 'ST034', 'PA195', 'ST230']\n",
      "ğŸ“‹ Exemples de variables catÃ©gorielles : ['CYC', 'NatCen', 'SUBNATIO', 'OECD', 'ADMINMODE', 'LANGTEST_QQQ', 'LANGTEST_COG', 'LANGTEST_PAQ', 'Option_CT', 'Option_FL']\n"
     ]
    }
   ],
   "source": [
    "# Extraire les listes de variables depuis les colonnes\n",
    "# Chaque colonne contient une liste de codes de variables\n",
    "\n",
    "# Variables ordinales : toutes les valeurs non-nulles de la colonne \"Variables Ordinales\"\n",
    "ordinal_vars = classification['Variables Ordinales'].drop_nulls().to_list()\n",
    "\n",
    "# Variables catÃ©gorielles : toutes les valeurs non-nulles de la colonne \"Variables CatÃ©gorielles\"\n",
    "categorical_vars = classification['Variables CatÃ©gorielles'].drop_nulls().to_list()\n",
    "\n",
    "# Variables numÃ©riques : toutes les valeurs non-nulles de la colonne \"Variables NumÃ©riques\"\n",
    "numeric_vars = classification['Variables NumÃ©riques'].drop_nulls().to_list()\n",
    "\n",
    "# Variables de groupement : toutes les valeurs non-nulles de la colonne \"Variables de Groupement\"\n",
    "grouping_vars = classification['Variables de Groupement'].drop_nulls().to_list()\n",
    "\n",
    "print(f\"âœ… Variables ordinales : {len(ordinal_vars)}\")\n",
    "print(f\"âœ… Variables catÃ©gorielles : {len(categorical_vars)}\")\n",
    "print(f\"âœ… Variables numÃ©riques : {len(numeric_vars)}\")\n",
    "print(f\"âœ… Variables de groupement : {len(grouping_vars)}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Exemples de variables ordinales : {ordinal_vars[:10]}\")\n",
    "print(f\"ğŸ“‹ Exemples de variables catÃ©gorielles : {categorical_vars[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š PRÃ‰SENCE DANS LE DATASET\n",
      "Variables ordinales prÃ©sentes : 95 / 96\n",
      "Variables catÃ©gorielles prÃ©sentes : 70 / 70\n",
      "\n",
      "âš ï¸ Variables ordinales manquantes : 1\n",
      "Exemples : ['MathScore']\n",
      "\n",
      "âœ… Listes mises Ã  jour avec variables prÃ©sentes dans le dataset\n",
      "Ordinales : 95\n",
      "CatÃ©gorielles : 70\n"
     ]
    }
   ],
   "source": [
    "# VÃ©rifier quelles variables de la classification sont prÃ©sentes dans le dataset\n",
    "dataset_cols = set(df.columns)\n",
    "ordinal_in_dataset = [v for v in ordinal_vars if v in dataset_cols]\n",
    "categorical_in_dataset = [v for v in categorical_vars if v in dataset_cols]\n",
    "\n",
    "print(f\"\\nğŸ“Š PRÃ‰SENCE DANS LE DATASET\")\n",
    "print(f\"Variables ordinales prÃ©sentes : {len(ordinal_in_dataset)} / {len(ordinal_vars)}\")\n",
    "print(f\"Variables catÃ©gorielles prÃ©sentes : {len(categorical_in_dataset)} / {len(categorical_vars)}\")\n",
    "\n",
    "# Variables manquantes\n",
    "ordinal_missing = [v for v in ordinal_vars if v not in dataset_cols]\n",
    "categorical_missing = [v for v in categorical_vars if v not in dataset_cols]\n",
    "\n",
    "if ordinal_missing:\n",
    "    print(f\"\\nâš ï¸ Variables ordinales manquantes : {len(ordinal_missing)}\")\n",
    "    print(f\"Exemples : {ordinal_missing[:5]}\")\n",
    "\n",
    "if categorical_missing:\n",
    "    print(f\"\\nâš ï¸ Variables catÃ©gorielles manquantes : {len(categorical_missing)}\")\n",
    "    print(f\"Exemples : {categorical_missing[:5]}\")\n",
    "\n",
    "# Mettre Ã  jour les listes pour ne garder que les variables prÃ©sentes\n",
    "ordinal_vars = ordinal_in_dataset\n",
    "categorical_vars = categorical_in_dataset\n",
    "\n",
    "print(f\"\\nâœ… Listes mises Ã  jour avec variables prÃ©sentes dans le dataset\")\n",
    "print(f\"Ordinales : {len(ordinal_vars)}\")\n",
    "print(f\"CatÃ©gorielles : {len(categorical_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est normal, c'est le label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des variables ordinales\n",
    "\n",
    "Pour chaque variable ordinale :\n",
    "- CardinalitÃ© (nombre de valeurs uniques)\n",
    "- % de valeurs manquantes\n",
    "- Type d'Ã©chelle (Likert, frÃ©quence, quantitÃ©)\n",
    "- Valeurs aberrantes ou codes spÃ©ciaux (-99, 97, 98, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction analyze_ordinal_variable dÃ©finie\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour analyser une variable ordinale avec Polars\n",
    "def analyze_ordinal_variable(df: pl.DataFrame, var_name: str) -> dict:\n",
    "    \"\"\"Analyse une variable ordinale avec Polars.\"\"\"\n",
    "    if var_name not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    col = df[var_name]\n",
    "    \n",
    "    # Statistiques de base\n",
    "    n_unique = col.n_unique()\n",
    "    missing_count = col.null_count()\n",
    "    missing_pct = (missing_count / df.height) * 100\n",
    "    \n",
    "    # Valeurs uniques (limitÃ©es Ã  20) - convertir en strings pour Ã©viter problÃ¨mes de types mixtes\n",
    "    unique_vals_raw = col.drop_nulls().unique().to_list()\n",
    "    unique_vals = str(sorted(unique_vals_raw)[:20]) if unique_vals_raw else \"[]\"\n",
    "    \n",
    "    # Value counts (top 10) - convertir en string pour Ã©viter problÃ¨mes avec None/null\n",
    "    value_counts = df.group_by(var_name).agg(pl.count().alias('count')).sort('count', descending=True).head(10)\n",
    "    value_counts_dict = str({str(row[0]): row[1] for row in value_counts.iter_rows()})\n",
    "    \n",
    "    # Codes spÃ©ciaux (si numÃ©rique)\n",
    "    if col.dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
    "        has_negative = int((col < 0).sum())\n",
    "        has_high_codes = int(((col >= 97) & (col <= 99)).sum())\n",
    "    else:\n",
    "        has_negative = 0\n",
    "        has_high_codes = 0\n",
    "    \n",
    "    analysis = {\n",
    "        'variable': var_name,\n",
    "        'n_unique': int(n_unique),\n",
    "        'missing_count': int(missing_count),\n",
    "        'missing_pct': float(missing_pct),\n",
    "        'dtype': str(col.dtype),\n",
    "        'unique_values': unique_vals,\n",
    "        'value_counts': value_counts_dict,\n",
    "        'has_negative_codes': has_negative,\n",
    "        'has_high_codes': has_high_codes\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"âœ… Fonction analyze_ordinal_variable dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse de 95 variables ordinales...\n",
      "\n",
      "âœ… Analyse de 95 variables ordinales terminÃ©e\n"
     ]
    }
   ],
   "source": [
    "# Analyser toutes les variables ordinales\n",
    "if 'ordinal_vars' in locals() and len(ordinal_vars) > 0:\n",
    "    ordinal_analysis = []\n",
    "    \n",
    "    print(f\"Analyse de {len(ordinal_vars)} variables ordinales...\")\n",
    "    for var in ordinal_vars:\n",
    "        result = analyze_ordinal_variable(df, var)\n",
    "        if result:\n",
    "            ordinal_analysis.append(result)\n",
    "    \n",
    "    # Convertir en DataFrame Polars\n",
    "    ordinal_df = pl.DataFrame(ordinal_analysis)\n",
    "    print(f\"\\nâœ… Analyse de {len(ordinal_df)} variables ordinales terminÃ©e\")\n",
    "    ordinal_df.head(10)\n",
    "else:\n",
    "    print(\"âš ï¸ Liste des variables ordinales non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š STATISTIQUES VARIABLES ORDINALES\n",
      "============================================================\n",
      "Nombre total : 95\n",
      "\n",
      "Missing values :\n",
      "  - Moyenne : 58.68%\n",
      "  - MÃ©diane : 64.92%\n",
      "  - Max : 69.91%\n",
      "\n",
      "CardinalitÃ© :\n",
      "  - Moyenne : 27.1\n",
      "  - MÃ©diane : 22.0\n",
      "  - Max : 148\n",
      "\n",
      "Codes spÃ©ciaux :\n",
      "  - Variables avec codes nÃ©gatifs : 0\n",
      "  - Variables avec codes 97-99 : 0\n"
     ]
    }
   ],
   "source": [
    "# Statistiques sur les variables ordinales\n",
    "if 'ordinal_df' in locals():\n",
    "    print(\"\\nğŸ“Š STATISTIQUES VARIABLES ORDINALES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Nombre total : {ordinal_df.height}\")\n",
    "    print(f\"\\nMissing values :\")\n",
    "    print(f\"  - Moyenne : {ordinal_df['missing_pct'].mean():.2f}%\")\n",
    "    print(f\"  - MÃ©diane : {ordinal_df['missing_pct'].median():.2f}%\")\n",
    "    print(f\"  - Max : {ordinal_df['missing_pct'].max():.2f}%\")\n",
    "    print(f\"\\nCardinalitÃ© :\")\n",
    "    print(f\"  - Moyenne : {ordinal_df['n_unique'].mean():.1f}\")\n",
    "    print(f\"  - MÃ©diane : {ordinal_df['n_unique'].median():.1f}\")\n",
    "    print(f\"  - Max : {ordinal_df['n_unique'].max()}\")\n",
    "    print(f\"\\nCodes spÃ©ciaux :\")\n",
    "    print(f\"  - Variables avec codes nÃ©gatifs : {(ordinal_df['has_negative_codes'] > 0).sum()}\")\n",
    "    print(f\"  - Variables avec codes 97-99 : {(ordinal_df['has_high_codes'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des variables catÃ©gorielles\n",
    "\n",
    "Pour chaque variable catÃ©gorielle :\n",
    "- CardinalitÃ© (faible < 10, moyenne 10-50, haute > 50)\n",
    "- % de valeurs manquantes\n",
    "- CatÃ©gories rares (< 1%)\n",
    "- DÃ©sÃ©quilibre (imbalance ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction analyze_categorical_variable dÃ©finie\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour analyser une variable catÃ©gorielle avec Polars\n",
    "def analyze_categorical_variable(df: pl.DataFrame, var_name: str) -> dict:\n",
    "    \"\"\"Analyse une variable catÃ©gorielle avec Polars.\"\"\"\n",
    "    if var_name not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    col = df[var_name]\n",
    "    \n",
    "    # Statistiques de base\n",
    "    cardinality = col.n_unique()\n",
    "    missing_count = col.null_count()\n",
    "    missing_pct = (missing_count / df.height) * 100\n",
    "    \n",
    "    # Value counts\n",
    "    value_counts = df.group_by(var_name).agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "    \n",
    "    # CatÃ©gories rares (< 1%)\n",
    "    total = df.height - missing_count\n",
    "    rare_threshold = 0.01 * total\n",
    "    rare_categories = (value_counts.filter(pl.col('count') < rare_threshold)).height\n",
    "    \n",
    "    # Imbalance ratio\n",
    "    if value_counts.height > 0:\n",
    "        max_count = value_counts['count'][0]\n",
    "        min_count = value_counts['count'][-1]\n",
    "        imbalance_ratio = float(max_count / min_count) if min_count > 0 else float('inf')\n",
    "    else:\n",
    "        imbalance_ratio = 0.0\n",
    "    \n",
    "    # Top 5 categories - convertir en string pour Ã©viter problÃ¨mes avec None/null\n",
    "    top_5 = value_counts.head(5)\n",
    "    top_5_dict = str({str(row[0]): row[1] for row in top_5.iter_rows()})\n",
    "    \n",
    "    # Type de cardinalitÃ©\n",
    "    if cardinality < 10:\n",
    "        card_type = 'faible'\n",
    "    elif cardinality <= 50:\n",
    "        card_type = 'moyenne'\n",
    "    else:\n",
    "        card_type = 'haute'\n",
    "    \n",
    "    analysis = {\n",
    "        'variable': var_name,\n",
    "        'cardinality': int(cardinality),\n",
    "        'missing_count': int(missing_count),\n",
    "        'missing_pct': float(missing_pct),\n",
    "        'dtype': str(col.dtype),\n",
    "        'rare_categories': int(rare_categories),\n",
    "        'imbalance_ratio': float(imbalance_ratio),\n",
    "        'top_5_categories': top_5_dict,\n",
    "        'cardinality_type': card_type\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"âœ… Fonction analyze_categorical_variable dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse de 70 variables catÃ©gorielles...\n",
      "\n",
      "âœ… Analyse de 70 variables catÃ©gorielles terminÃ©e\n"
     ]
    }
   ],
   "source": [
    "# Analyser toutes les variables catÃ©gorielles\n",
    "if 'categorical_vars' in locals() and len(categorical_vars) > 0:\n",
    "    categorical_analysis = []\n",
    "    \n",
    "    print(f\"Analyse de {len(categorical_vars)} variables catÃ©gorielles...\")\n",
    "    for var in categorical_vars:\n",
    "        result = analyze_categorical_variable(df, var)\n",
    "        if result:\n",
    "            categorical_analysis.append(result)\n",
    "    \n",
    "    # Convertir en DataFrame Polars\n",
    "    categorical_df = pl.DataFrame(categorical_analysis)\n",
    "    print(f\"\\nâœ… Analyse de {len(categorical_df)} variables catÃ©gorielles terminÃ©e\")\n",
    "    categorical_df.head(10)\n",
    "else:\n",
    "    print(\"âš ï¸ Liste des variables catÃ©gorielles non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š STATISTIQUES VARIABLES CATÃ‰GORIELLES\n",
      "============================================================\n",
      "Nombre total : 70\n",
      "\n",
      "Missing values :\n",
      "  - Moyenne : 47.93%\n",
      "  - MÃ©diane : 64.92%\n",
      "  - Max : 83.61%\n",
      "\n",
      "CardinalitÃ© :\n",
      "  - Moyenne : 48.7\n",
      "  - MÃ©diane : 11.0\n",
      "  - Max : 707\n",
      "\n",
      "Distribution par type de cardinalitÃ© :\n",
      "shape: (3, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ cardinality_type â”† count â”‚\n",
      "â”‚ ---              â”† ---   â”‚\n",
      "â”‚ str              â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ moyenne          â”† 28    â”‚\n",
      "â”‚ faible           â”† 31    â”‚\n",
      "â”‚ haute            â”† 11    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "CatÃ©gories rares :\n",
      "  - Variables avec catÃ©gories rares : 39\n",
      "  - Moyenne catÃ©gories rares par variable : 39.0\n"
     ]
    }
   ],
   "source": [
    "# Statistiques sur les variables catÃ©gorielles\n",
    "if 'categorical_df' in locals():\n",
    "    print(\"\\nğŸ“Š STATISTIQUES VARIABLES CATÃ‰GORIELLES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Nombre total : {categorical_df.height}\")\n",
    "    print(f\"\\nMissing values :\")\n",
    "    print(f\"  - Moyenne : {categorical_df['missing_pct'].mean():.2f}%\")\n",
    "    print(f\"  - MÃ©diane : {categorical_df['missing_pct'].median():.2f}%\")\n",
    "    print(f\"  - Max : {categorical_df['missing_pct'].max():.2f}%\")\n",
    "    print(f\"\\nCardinalitÃ© :\")\n",
    "    print(f\"  - Moyenne : {categorical_df['cardinality'].mean():.1f}\")\n",
    "    print(f\"  - MÃ©diane : {categorical_df['cardinality'].median():.1f}\")\n",
    "    print(f\"  - Max : {categorical_df['cardinality'].max()}\")\n",
    "    print(f\"\\nDistribution par type de cardinalitÃ© :\")\n",
    "    card_dist = categorical_df.group_by('cardinality_type').agg(pl.count().alias('count'))\n",
    "    print(card_dist)\n",
    "    print(f\"\\nCatÃ©gories rares :\")\n",
    "    print(f\"  - Variables avec catÃ©gories rares : {(categorical_df['rare_categories'] > 0).sum()}\")\n",
    "    print(f\"  - Moyenne catÃ©gories rares par variable : {categorical_df['rare_categories'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Focus : Variables Ã  haute cardinalitÃ©\n",
    "\n",
    "Analyse approfondie de STRATUM, OCOD, CNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSE DE STRATUM\n",
      "============================================================\n",
      "\n",
      "CardinalitÃ© : 2472\n",
      "Missing : 0.00%\n",
      "CatÃ©gories rares : 2469\n",
      "\n",
      "Top 10 valeurs STRATUM :\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ STRATUM â”† count â”‚\n",
      "â”‚ ---     â”† ---   â”‚\n",
      "â”‚ str     â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ CAN99   â”† 655   â”‚\n",
      "â”‚ ITA9797 â”† 643   â”‚\n",
      "â”‚ THA9797 â”† 500   â”‚\n",
      "â”‚ AUT9797 â”† 397   â”‚\n",
      "â”‚ ISR9797 â”† 396   â”‚\n",
      "â”‚ DEU9797 â”† 368   â”‚\n",
      "â”‚ QCI9797 â”† 330   â”‚\n",
      "â”‚ NOR0001 â”† 320   â”‚\n",
      "â”‚ ITA97   â”† 308   â”‚\n",
      "â”‚ GRC0003 â”† 278   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Analyser STRATUM (1316 strates attendues)\n",
    "if 'STRATUM' in df.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANALYSE DE STRATUM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stratum_analysis = analyze_categorical_variable(df, 'STRATUM')\n",
    "    print(f\"\\nCardinalitÃ© : {stratum_analysis['cardinality']}\")\n",
    "    print(f\"Missing : {stratum_analysis['missing_pct']:.2f}%\")\n",
    "    print(f\"CatÃ©gories rares : {stratum_analysis['rare_categories']}\")\n",
    "    \n",
    "    # Ã‰chantillon de valeurs\n",
    "    print(\"\\nTop 10 valeurs STRATUM :\")\n",
    "    stratum_counts = df.group_by('STRATUM').agg(pl.count().alias('count')).sort('count', descending=True).head(10)\n",
    "    print(stratum_counts)\n",
    "else:\n",
    "    print(\"âš ï¸ STRATUM non trouvÃ© dans le dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables OCOD trouvÃ©es : ['OCOD1', 'OCOD2', 'OCOD3']\n",
      "\n",
      "============================================================\n",
      "ANALYSE DE OCOD1 (Professions)\n",
      "============================================================\n",
      "\n",
      "CardinalitÃ© : 665\n",
      "Missing : 30.09%\n",
      "\n",
      "Top 10 professions :\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ OCOD1  â”† count â”‚\n",
      "â”‚ ---    â”† ---   â”‚\n",
      "â”‚ f64    â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ null   â”† 15047 â”‚\n",
      "â”‚ 9701.0 â”† 5064  â”‚\n",
      "â”‚ 9999.0 â”† 3503  â”‚\n",
      "â”‚ 9705.0 â”† 1009  â”‚\n",
      "â”‚ 5223.0 â”† 829   â”‚\n",
      "â”‚ 2330.0 â”† 745   â”‚\n",
      "â”‚ 5120.0 â”† 745   â”‚\n",
      "â”‚ 2221.0 â”† 712   â”‚\n",
      "â”‚ 2341.0 â”† 704   â”‚\n",
      "â”‚ 9703.0 â”† 626   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "ANALYSE DE OCOD2 (Professions)\n",
      "============================================================\n",
      "\n",
      "CardinalitÃ© : 707\n",
      "Missing : 30.09%\n",
      "\n",
      "Top 10 professions :\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ OCOD2  â”† count â”‚\n",
      "â”‚ ---    â”† ---   â”‚\n",
      "â”‚ f64    â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ null   â”† 15047 â”‚\n",
      "â”‚ 9999.0 â”† 4317  â”‚\n",
      "â”‚ 9705.0 â”† 1569  â”‚\n",
      "â”‚ 8332.0 â”† 757   â”‚\n",
      "â”‚ 8322.0 â”† 725   â”‚\n",
      "â”‚ 9703.0 â”† 583   â”‚\n",
      "â”‚ 7111.0 â”† 576   â”‚\n",
      "â”‚ 9704.0 â”† 553   â”‚\n",
      "â”‚ 7231.0 â”† 535   â”‚\n",
      "â”‚ 9998.0 â”† 466   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "ANALYSE DE OCOD3 (Professions)\n",
      "============================================================\n",
      "\n",
      "CardinalitÃ© : 589\n",
      "Missing : 30.09%\n",
      "\n",
      "Top 10 professions :\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ OCOD3  â”† count â”‚\n",
      "â”‚ ---    â”† ---   â”‚\n",
      "â”‚ f64    â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ null   â”† 15047 â”‚\n",
      "â”‚ 9999.0 â”† 6505  â”‚\n",
      "â”‚ 9704.0 â”† 1765  â”‚\n",
      "â”‚ 9705.0 â”† 1678  â”‚\n",
      "â”‚ 2211.0 â”† 1096  â”‚\n",
      "â”‚ 2611.0 â”† 982   â”‚\n",
      "â”‚ 2212.0 â”† 826   â”‚\n",
      "â”‚ 5412.0 â”† 780   â”‚\n",
      "â”‚ 2634.0 â”† 607   â”‚\n",
      "â”‚ 9998.0 â”† 550   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Analyser OCOD (620 professions attendues)\n",
    "# Note: OCOD peut avoir plusieurs variantes (OCOD1, OCOD2, OCOD3)\n",
    "ocod_vars = [col for col in df.columns if 'OCOD' in col]\n",
    "print(f\"Variables OCOD trouvÃ©es : {ocod_vars}\")\n",
    "\n",
    "for ocod_var in ocod_vars:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ANALYSE DE {ocod_var} (Professions)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ocod_analysis = analyze_categorical_variable(df, ocod_var)\n",
    "    print(f\"\\nCardinalitÃ© : {ocod_analysis['cardinality']}\")\n",
    "    print(f\"Missing : {ocod_analysis['missing_pct']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nTop 10 professions :\")\n",
    "    ocod_counts = df.group_by(ocod_var).agg(pl.count().alias('count')).sort('count', descending=True).head(10)\n",
    "    print(ocod_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSE CNT vs CNTRYID (Redondance)\n",
      "============================================================\n",
      "\n",
      "CNT - CardinalitÃ© : 98\n",
      "CNTRYID - CardinalitÃ© : 99\n",
      "\n",
      "Mappings uniques CNT <-> CNTRYID : 99\n",
      "Redondance parfaite : False\n",
      "\n",
      "Exemples de mapping :\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ CNT â”† CNTRYID â”‚\n",
      "â”‚ --- â”† ---     â”‚\n",
      "â”‚ str â”† f64     â”‚\n",
      "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ SLV â”† 222.0   â”‚\n",
      "â”‚ ISR â”† 376.0   â”‚\n",
      "â”‚ DEU â”† 276.0   â”‚\n",
      "â”‚ PRT â”† 620.0   â”‚\n",
      "â”‚ BGR â”† 100.0   â”‚\n",
      "â”‚ CRI â”† 188.0   â”‚\n",
      "â”‚ NLD â”† 528.0   â”‚\n",
      "â”‚ TUR â”† 792.0   â”‚\n",
      "â”‚ ARG â”† 32.0    â”‚\n",
      "â”‚ KAZ â”† 398.0   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Analyser CNT vs CNTRYID (redondance potentielle)\n",
    "if 'CNT' in df.columns and 'CNTRYID' in df.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANALYSE CNT vs CNTRYID (Redondance)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cnt_analysis = analyze_categorical_variable(df, 'CNT')\n",
    "    cntryid_analysis = analyze_categorical_variable(df, 'CNTRYID')\n",
    "    \n",
    "    print(f\"\\nCNT - CardinalitÃ© : {cnt_analysis['cardinality']}\")\n",
    "    print(f\"CNTRYID - CardinalitÃ© : {cntryid_analysis['cardinality']}\")\n",
    "    \n",
    "    # VÃ©rifier la redondance\n",
    "    mapping = df.select(['CNT', 'CNTRYID']).unique()\n",
    "    print(f\"\\nMappings uniques CNT <-> CNTRYID : {mapping.height}\")\n",
    "    print(f\"Redondance parfaite : {mapping.height == cnt_analysis['cardinality']}\")\n",
    "    \n",
    "    print(\"\\nExemples de mapping :\")\n",
    "    print(mapping.head(10))\n",
    "else:\n",
    "    print(\"âš ï¸ CNT ou CNTRYID non trouvÃ© dans le dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DÃ©tection de types d'Ã©chelles ordinales\n",
    "\n",
    "Identifier :\n",
    "- Ã‰chelles de Likert (3, 4, 5, 7 points)\n",
    "- Ã‰chelles de frÃ©quence\n",
    "- Ã‰chelles de quantitÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction detect_scale_type dÃ©finie\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour dÃ©tecter le type d'Ã©chelle\n",
    "def detect_scale_type(unique_values, n_unique):\n",
    "    \"\"\"DÃ©tecte le type d'Ã©chelle ordinale.\"\"\"\n",
    "    \n",
    "    # Ã‰chelles de Likert\n",
    "    if n_unique in [3, 4, 5, 7]:\n",
    "        # VÃ©rifier si les valeurs sont consÃ©cutives (ex: 1,2,3,4,5)\n",
    "        if all(isinstance(v, (int, np.integer)) for v in unique_values):\n",
    "            sorted_vals = sorted(unique_values)\n",
    "            if sorted_vals == list(range(min(sorted_vals), max(sorted_vals) + 1)):\n",
    "                return f'likert_{n_unique}'\n",
    "    \n",
    "    # Ã‰chelles avec valeurs nÃ©gatives (codes spÃ©ciaux)\n",
    "    if any(v < 0 for v in unique_values if isinstance(v, (int, float, np.number))):\n",
    "        return 'with_special_codes'\n",
    "    \n",
    "    # Ã‰chelles de quantitÃ© par ranges\n",
    "    if n_unique >= 5 and n_unique <= 10:\n",
    "        return 'quantity_ranges'\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "print(\"âœ… Fonction detect_scale_type dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution des types d'Ã©chelles ordinales :\n",
      "shape: (2, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ scale_type      â”† count â”‚\n",
      "â”‚ ---             â”† ---   â”‚\n",
      "â”‚ str             â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ other           â”† 76    â”‚\n",
      "â”‚ quantity_ranges â”† 19    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Exemples par type d'Ã©chelle :\n",
      "\n",
      "QUANTITY_RANGES :\n",
      "shape: (3, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ variable â”† n_unique â”† unique_values                   â”‚\n",
      "â”‚ ---      â”† ---      â”† ---                             â”‚\n",
      "â”‚ str      â”† i64      â”† str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ ISCEDP   â”† 8        â”† [244.0, 254.0, 341.0, 343.0, 3â€¦ â”‚\n",
      "â”‚ ST253    â”† 9        â”† [0.0, 0.1428571428571428, 0.28â€¦ â”‚\n",
      "â”‚ PA195    â”† 8        â”† [0.0, 0.1666666666666666, 0.33â€¦ â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "OTHER :\n",
      "shape: (3, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ variable â”† n_unique â”† unique_values                   â”‚\n",
      "â”‚ ---      â”† ---      â”† ---                             â”‚\n",
      "â”‚ str      â”† i64      â”† str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ ST311    â”† 22       â”† [0.0, 0.25, 0.5, 0.75, 1.0, 1.â€¦ â”‚\n",
      "â”‚ IC171    â”† 144      â”† [0.0, 0.2, 0.25, 0.33333333333â€¦ â”‚\n",
      "â”‚ ST038    â”† 43       â”† [0.0, 0.3333333333333333, 0.66â€¦ â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# DÃ©tecter types d'Ã©chelles pour les variables ordinales\n",
    "if 'ordinal_df' in locals():\n",
    "    # Ajouter colonne scale_type\n",
    "    scale_types = []\n",
    "    for row in ordinal_df.iter_rows(named=True):\n",
    "        scale_type = detect_scale_type(row['unique_values'], row['n_unique'])\n",
    "        scale_types.append(scale_type)\n",
    "    \n",
    "    ordinal_df = ordinal_df.with_columns(pl.Series('scale_type', scale_types))\n",
    "    \n",
    "    print(\"\\nDistribution des types d'Ã©chelles ordinales :\")\n",
    "    scale_dist = ordinal_df.group_by('scale_type').agg(pl.count().alias('count'))\n",
    "    print(scale_dist)\n",
    "    \n",
    "    print(\"\\nExemples par type d'Ã©chelle :\")\n",
    "    for scale_type in ordinal_df['scale_type'].unique().to_list():\n",
    "        print(f\"\\n{scale_type.upper()} :\")\n",
    "        examples = ordinal_df.filter(pl.col('scale_type') == scale_type).select(['variable', 'n_unique', 'unique_values']).head(3)\n",
    "        print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des valeurs manquantes\n",
    "\n",
    "Patterns de missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 variables avec le plus de valeurs manquantes :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>variable</th><th>missing_count</th><th>missing_pct</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;reading_q13_average_score&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;reading_q14_average_score&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;reading_q15_average_score&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;math_q17_average_score&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;math_q18_average_score&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;reading_q13_total_timing&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;reading_q14_total_timing&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;reading_q15_total_timing&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;math_q17_total_timing&quot;</td><td>50000</td><td>100.0</td></tr><tr><td>&quot;math_q18_total_timing&quot;</td><td>50000</td><td>100.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ variable                  â”† missing_count â”† missing_pct â”‚\n",
       "â”‚ ---                       â”† ---           â”† ---         â”‚\n",
       "â”‚ str                       â”† i64           â”† f64         â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ reading_q13_average_score â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ reading_q14_average_score â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ reading_q15_average_score â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ math_q17_average_score    â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ math_q18_average_score    â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ â€¦                         â”† â€¦             â”† â€¦           â”‚\n",
       "â”‚ reading_q13_total_timing  â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ reading_q14_total_timing  â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ reading_q15_total_timing  â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ math_q17_total_timing     â”† 50000         â”† 100.0       â”‚\n",
       "â”‚ math_q18_total_timing     â”† 50000         â”† 100.0       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistiques globales sur les valeurs manquantes avec Polars\n",
    "missing_stats = pl.DataFrame({\n",
    "    'variable': df.columns,\n",
    "    'missing_count': [df[col].null_count() for col in df.columns],\n",
    "    'missing_pct': [(df[col].null_count() / df.height) * 100 for col in df.columns]\n",
    "}).sort('missing_pct', descending=True)\n",
    "\n",
    "print(\"Top 20 variables avec le plus de valeurs manquantes :\")\n",
    "missing_stats.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RÃ©sumÃ© et conclusions prÃ©liminaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RÃ‰SUMÃ‰ DE L'ANALYSE EXPLORATOIRE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š DATASET\n",
      "  - Taille de l'Ã©chantillon : 50,000 lignes\n",
      "  - Nombre de colonnes : 307\n",
      "  - MÃ©moire : 102.50 MB\n",
      "\n",
      "ğŸ“ˆ VARIABLES ORDINALES\n",
      "  - Nombre : 95\n",
      "  - Missing moyen : 58.68%\n",
      "  - CardinalitÃ© moyenne : 27.1\n",
      "\n",
      "ğŸ·ï¸  VARIABLES CATÃ‰GORIELLES\n",
      "  - Nombre : 70\n",
      "  - Missing moyen : 47.93%\n",
      "  - CardinalitÃ© moyenne : 48.7\n",
      "  - Haute cardinalitÃ© (>50) : 11\n",
      "\n",
      "âš ï¸  VARIABLES Ã€ FORTE MISSING (>50%)\n",
      "  - Nombre : 207\n",
      "  - Ã€ considÃ©rer pour suppression\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# RÃ©sumÃ© des dÃ©couvertes\n",
    "print(\"=\" * 80)\n",
    "print(\"RÃ‰SUMÃ‰ DE L'ANALYSE EXPLORATOIRE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nğŸ“Š DATASET\")\n",
    "print(f\"  - Taille de l'Ã©chantillon : {df.height:,} lignes\")\n",
    "print(f\"  - Nombre de colonnes : {df.width}\")\n",
    "print(f\"  - MÃ©moire : {df.estimated_size('mb'):.2f} MB\")\n",
    "\n",
    "if 'ordinal_df' in locals():\n",
    "    print(f\"\\nğŸ“ˆ VARIABLES ORDINALES\")\n",
    "    print(f\"  - Nombre : {ordinal_df.height}\")\n",
    "    print(f\"  - Missing moyen : {ordinal_df['missing_pct'].mean():.2f}%\")\n",
    "    print(f\"  - CardinalitÃ© moyenne : {ordinal_df['n_unique'].mean():.1f}\")\n",
    "\n",
    "if 'categorical_df' in locals():\n",
    "    print(f\"\\nğŸ·ï¸  VARIABLES CATÃ‰GORIELLES\")\n",
    "    print(f\"  - Nombre : {categorical_df.height}\")\n",
    "    print(f\"  - Missing moyen : {categorical_df['missing_pct'].mean():.2f}%\")\n",
    "    print(f\"  - CardinalitÃ© moyenne : {categorical_df['cardinality'].mean():.1f}\")\n",
    "    print(f\"  - Haute cardinalitÃ© (>50) : {(categorical_df['cardinality'] > 50).sum()}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  VARIABLES Ã€ FORTE MISSING (>50%)\")\n",
    "high_missing = missing_stats.filter(pl.col('missing_pct') > 50)\n",
    "print(f\"  - Nombre : {high_missing.height}\")\n",
    "print(f\"  - Ã€ considÃ©rer pour suppression\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. DÃ©tection de variables redondantes / corrÃ©lÃ©es\n",
    "\n",
    "Objectifs :\n",
    "- Calculer corrÃ©lations de Spearman pour paires de variables ordinales\n",
    "- Calculer CramÃ©r's V pour paires de variables catÃ©gorielles  \n",
    "- Identifier variables dupliquÃ©es\n",
    "- **Output** : Liste de variables Ã  supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction cramers_v dÃ©finie\n"
     ]
    }
   ],
   "source": [
    "# 10.1 - Fonction pour calculer CramÃ©r's V entre deux variables catÃ©gorielles\n",
    "def cramers_v(df: pl.DataFrame, var1: str, var2: str) -> float:\n",
    "    \"\"\"Calcule le coefficient de CramÃ©r's V entre deux variables catÃ©gorielles.\"\"\"\n",
    "    # CrÃ©er une table de contingence avec Polars\n",
    "    contingency = df.group_by([var1, var2]).agg(pl.count().alias('count'))\n",
    "    \n",
    "    # Convertir en pandas pour utiliser chi2_contingency\n",
    "    contingency_pd = contingency.to_pandas()\n",
    "    ct = pd.crosstab(\n",
    "        df[var1].to_pandas(), \n",
    "        df[var2].to_pandas()\n",
    "    )\n",
    "    \n",
    "    # Calculer chi2\n",
    "    chi2, _, _, _ = chi2_contingency(ct)\n",
    "    n = ct.sum().sum()\n",
    "    \n",
    "    # CramÃ©r's V\n",
    "    min_dim = min(ct.shape[0] - 1, ct.shape[1] - 1)\n",
    "    if min_dim == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    cramers = np.sqrt(chi2 / (n * min_dim))\n",
    "    return cramers\n",
    "\n",
    "print(\"âœ… Fonction cramers_v dÃ©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables avec '_duplicated' dans le nom : ['ST008_duplicated_0', 'ST006_duplicated_0', 'PA008_duplicated_0']\n",
      "\n",
      "âœ… Variables redondantes identifiÃ©es : []\n"
     ]
    }
   ],
   "source": [
    "# 10.2 - DÃ©tecter variables dupliquÃ©es (ST006, ST008, PA008)\n",
    "# Selon glossaire_analysis.md, il y a des codes dupliquÃ©s identifiÃ©s\n",
    "\n",
    "duplicated_candidates = [col for col in df.columns if 'duplicated' in col.lower()]\n",
    "print(f\"Variables avec '_duplicated' dans le nom : {duplicated_candidates}\")\n",
    "\n",
    "# VÃ©rifier les variables ST006, ST008, PA008 et leurs duplicatas\n",
    "vars_to_check = {\n",
    "    'ST006': ['ST006', 'ST006_duplicated_0'],\n",
    "    'ST008': ['ST008', 'ST008_duplicated_0'],\n",
    "    'PA008': ['PA008', 'PA008_duplicated_0']\n",
    "}\n",
    "\n",
    "redundant_vars = []\n",
    "\n",
    "for original, versions in vars_to_check.items():\n",
    "    available = [v for v in versions if v in df.columns]\n",
    "    \n",
    "    if len(available) == 2:\n",
    "        v1, v2 = available\n",
    "        \n",
    "        # Calculer corrÃ©lation de Spearman (pour variables ordinales/numÃ©riques)\n",
    "        # Convertir en pandas temporairement pour scipy\n",
    "        df_temp = df.select([v1, v2]).to_pandas()\n",
    "        \n",
    "        # Retirer les valeurs manquantes\n",
    "        df_temp_clean = df_temp.dropna()\n",
    "        \n",
    "        if len(df_temp_clean) > 0:\n",
    "            corr, pval = spearmanr(df_temp_clean[v1], df_temp_clean[v2])\n",
    "            \n",
    "            print(f\"\\n{original}:\")\n",
    "            print(f\"  - CorrÃ©lation Spearman : {corr:.4f} (p-value: {pval:.4e})\")\n",
    "            print(f\"  - Valeurs identiques : {(df[v1] == df[v2]).sum()} / {df.height}\")\n",
    "            \n",
    "            # Si corrÃ©lation trÃ¨s forte (>0.95), marquer comme redondant\n",
    "            if abs(corr) > 0.95:\n",
    "                redundant_vars.append(v2)  # Garder l'original, supprimer le duplicata\n",
    "                print(f\"  â¡ï¸ REDONDANT : Ã  supprimer {v2}\")\n",
    "\n",
    "print(f\"\\nâœ… Variables redondantes identifiÃ©es : {redundant_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSE REDONDANCE CNT vs CNTRYID\n",
      "============================================================\n",
      "\n",
      "CNT cardinalitÃ© : 98\n",
      "CNTRYID cardinalitÃ© : 99\n",
      "Mappings uniques : 99\n",
      "\n",
      "Mapping 1-to-1 parfait : False\n",
      "âš ï¸ Mapping imparfait dÃ©tectÃ© - vÃ©rifier manuellement\n",
      "\n",
      "âœ… Total variables redondantes : 0\n"
     ]
    }
   ],
   "source": [
    "# 10.3 - VÃ©rifier redondance CNT vs CNTRYID (dÃ©jÃ  identifiÃ© en section 6)\n",
    "# Ces deux variables encodent la mÃªme information (pays)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYSE REDONDANCE CNT vs CNTRYID\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# VÃ©rifier mapping 1-to-1\n",
    "mapping = df.select(['CNT', 'CNTRYID']).unique()\n",
    "cnt_unique = df['CNT'].n_unique()\n",
    "cntryid_unique = df['CNTRYID'].n_unique()\n",
    "\n",
    "print(f\"\\nCNT cardinalitÃ© : {cnt_unique}\")\n",
    "print(f\"CNTRYID cardinalitÃ© : {cntryid_unique}\")\n",
    "print(f\"Mappings uniques : {mapping.height}\")\n",
    "\n",
    "# VÃ©rifier si chaque CNT correspond Ã  un seul CNTRYID\n",
    "cnt_to_cntryid = df.group_by('CNT').agg(pl.col('CNTRYID').n_unique().alias('n_cntryid'))\n",
    "perfect_mapping = (cnt_to_cntryid['n_cntryid'] == 1).all()\n",
    "\n",
    "print(f\"\\nMapping 1-to-1 parfait : {perfect_mapping}\")\n",
    "\n",
    "if perfect_mapping:\n",
    "    print(\"â¡ï¸ REDONDANCE CONFIRMÃ‰E : CNT et CNTRYID encodent la mÃªme information\")\n",
    "    print(\"   Recommandation : garder CNT (plus lisible) et supprimer CNTRYID\")\n",
    "    redundant_vars.append('CNTRYID')\n",
    "else:\n",
    "    print(\"âš ï¸ Mapping imparfait dÃ©tectÃ© - vÃ©rifier manuellement\")\n",
    "\n",
    "print(f\"\\nâœ… Total variables redondantes : {len(redundant_vars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRÃ‰LATIONS ENTRE VARIABLES ORDINALES (Ã©chantillon)\n",
      "============================================================\n",
      "\n",
      "Variables ordinales avec cardinalitÃ© modÃ©rÃ©e et <70% missing : 85\n",
      "Exemples : ['ISCEDP', 'ST253', 'ST311', 'ST038', 'IC180', 'ST255', 'ST034', 'PA195', 'ST230', 'ST354']\n",
      "\n",
      "Matrice de corrÃ©lation (Spearman) pour 10 variables :\n",
      "          ISCEDP     ST253     ST311     ST038     IC180     ST255     ST034  \\\n",
      "ISCEDP  1.000000  0.045147  0.080035 -0.064641  0.016340  0.021478 -0.017170   \n",
      "ST253   0.045147  1.000000  0.164897  0.024930  0.191845  0.438111  0.031810   \n",
      "ST311   0.080035  0.164897  1.000000  0.093522  0.189675  0.145026  0.132295   \n",
      "ST038  -0.064641  0.024930  0.093522  1.000000 -0.005478  0.025445  0.196612   \n",
      "IC180   0.016340  0.191845  0.189675 -0.005478  1.000000  0.185604  0.026033   \n",
      "ST255   0.021478  0.438111  0.145026  0.025445  0.185604  1.000000  0.027535   \n",
      "ST034  -0.017170  0.031810  0.132295  0.196612  0.026033  0.027535  1.000000   \n",
      "PA195   0.115385  0.015889  0.060740 -0.050178  0.114866  0.047120 -0.019596   \n",
      "ST230  -0.065471 -0.009079  0.054586  0.094952 -0.054890 -0.074234  0.137148   \n",
      "ST354   0.036113  0.095938  0.170963  0.044544  0.151711  0.098431  0.113481   \n",
      "\n",
      "           PA195     ST230     ST354  \n",
      "ISCEDP  0.115385 -0.065471  0.036113  \n",
      "ST253   0.015889 -0.009079  0.095938  \n",
      "ST311   0.060740  0.054586  0.170963  \n",
      "ST038  -0.050178  0.094952  0.044544  \n",
      "IC180   0.114866 -0.054890  0.151711  \n",
      "ST255   0.047120 -0.074234  0.098431  \n",
      "ST034  -0.019596  0.137148  0.113481  \n",
      "PA195   1.000000 -0.053606  0.002996  \n",
      "ST230  -0.053606  1.000000  0.027669  \n",
      "ST354   0.002996  0.027669  1.000000  \n",
      "\n",
      "âœ… Pas de corrÃ©lations fortes dÃ©tectÃ©es dans cet Ã©chantillon\n",
      "\n",
      "Note: Analyse complÃ¨te nÃ©cessiterait calcul de toutes les paires\n"
     ]
    }
   ],
   "source": [
    "# 10.4 - Chercher corrÃ©lations fortes entre variables ordinales (Ã©chantillon)\n",
    "# Note: Calculer toutes les paires serait coÃ»teux (95*94/2 = 4465 paires)\n",
    "# On se concentre sur un Ã©chantillon ou les variables avec cardinalitÃ© modÃ©rÃ©e\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRÃ‰LATIONS ENTRE VARIABLES ORDINALES (Ã©chantillon)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SÃ©lectionner variables ordinales avec cardinalitÃ© modÃ©rÃ©e (5-50) et pas trop de missing\n",
    "ordinal_moderate = ordinal_df.filter(\n",
    "    (pl.col('n_unique') >= 5) & \n",
    "    (pl.col('n_unique') <= 50) &\n",
    "    (pl.col('missing_pct') < 70)\n",
    ")['variable'].to_list()\n",
    "\n",
    "print(f\"\\nVariables ordinales avec cardinalitÃ© modÃ©rÃ©e et <70% missing : {len(ordinal_moderate)}\")\n",
    "print(f\"Exemples : {ordinal_moderate[:10]}\")\n",
    "\n",
    "# Calculer matrice de corrÃ©lation pour un sous-ensemble (10 premiÃ¨res variables)\n",
    "if len(ordinal_moderate) > 0:\n",
    "    sample_vars = ordinal_moderate[:min(10, len(ordinal_moderate))]\n",
    "    \n",
    "    # Convertir en pandas pour corrÃ©lation\n",
    "    df_sample = df.select(sample_vars).to_pandas()\n",
    "    \n",
    "    # Calculer corrÃ©lation de Spearman\n",
    "    corr_matrix = df_sample.corr(method='spearman')\n",
    "    \n",
    "    print(f\"\\nMatrice de corrÃ©lation (Spearman) pour {len(sample_vars)} variables :\")\n",
    "    print(corr_matrix)\n",
    "    \n",
    "    # Identifier paires avec forte corrÃ©lation (>0.8, hors diagonale)\n",
    "    strong_corr = []\n",
    "    for i, var1 in enumerate(sample_vars):\n",
    "        for j, var2 in enumerate(sample_vars):\n",
    "            if i < j:  # Ã‰viter doublons et diagonale\n",
    "                corr_val = corr_matrix.loc[var1, var2]\n",
    "                if abs(corr_val) > 0.8 and not np.isnan(corr_val):\n",
    "                    strong_corr.append((var1, var2, corr_val))\n",
    "    \n",
    "    if strong_corr:\n",
    "        print(f\"\\nâš ï¸ Paires fortement corrÃ©lÃ©es (|r| > 0.8) :\")\n",
    "        for v1, v2, corr in strong_corr:\n",
    "            print(f\"  - {v1} <-> {v2} : {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Pas de corrÃ©lations fortes dÃ©tectÃ©es dans cet Ã©chantillon\")\n",
    "\n",
    "print(\"\\nNote: Analyse complÃ¨te nÃ©cessiterait calcul de toutes les paires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyse des patterns de valeurs manquantes\n",
    "\n",
    "Objectifs :\n",
    "- Identifier si les valeurs manquantes sont MCAR (Missing Completely At Random), MAR (Missing At Random), ou MNAR (Missing Not At Random)\n",
    "- DÃ©tecter corrÃ©lations entre valeurs manquantes de diffÃ©rentes variables\n",
    "- DÃ©cider stratÃ©gie d'imputation par variable\n",
    "- Identifier variables Ã  supprimer (trop de missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MATRICE DE VALEURS MANQUANTES\n",
      "============================================================\n",
      "\n",
      "Matrice crÃ©Ã©e : (50000, 162)\n",
      "Variables analysÃ©es : 165\n",
      "\n",
      "ğŸ“Š Statistiques globales :\n",
      "  - Total cellules : 8,100,000\n",
      "  - Cellules manquantes : 4,367,177\n",
      "  - Pourcentage global : 53.92%\n"
     ]
    }
   ],
   "source": [
    "# 11.1 - CrÃ©er matrice binaire des valeurs manquantes\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRICE DE VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CrÃ©er une matrice binaire : 1 si manquant, 0 sinon\n",
    "# On se concentre sur variables ordinales et catÃ©gorielles (pas toutes les 307 colonnes)\n",
    "vars_of_interest = ordinal_vars + categorical_vars\n",
    "\n",
    "missing_matrix = pl.DataFrame({\n",
    "    col: df[col].is_null().cast(pl.Int8)\n",
    "    for col in vars_of_interest\n",
    "})\n",
    "\n",
    "print(f\"\\nMatrice crÃ©Ã©e : {missing_matrix.shape}\")\n",
    "print(f\"Variables analysÃ©es : {len(vars_of_interest)}\")\n",
    "\n",
    "# Statistiques de base\n",
    "total_cells = missing_matrix.height * missing_matrix.width\n",
    "total_missing = sum([missing_matrix[col].sum() for col in missing_matrix.columns])\n",
    "missing_pct_overall = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Statistiques globales :\")\n",
    "print(f\"  - Total cellules : {total_cells:,}\")\n",
    "print(f\"  - Cellules manquantes : {total_missing:,}\")\n",
    "print(f\"  - Pourcentage global : {missing_pct_overall:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VARIABLES AVEC >80% DE VALEURS MANQUANTES\n",
      "============================================================\n",
      "\n",
      "Nombre total : 51\n",
      "\n",
      "PremiÃ¨res 20 variables :\n",
      "  - reading_q13_average_score: 100.00%\n",
      "  - reading_q14_average_score: 100.00%\n",
      "  - reading_q15_average_score: 100.00%\n",
      "  - math_q17_average_score: 100.00%\n",
      "  - math_q18_average_score: 100.00%\n",
      "  - math_q19_average_score: 100.00%\n",
      "  - math_q20_average_score: 100.00%\n",
      "  - math_q21_average_score: 100.00%\n",
      "  - science_q13_average_score: 100.00%\n",
      "  - science_q14_average_score: 100.00%\n",
      "  - science_q15_average_score: 100.00%\n",
      "  - science_q16_average_score: 100.00%\n",
      "  - science_q17_average_score: 100.00%\n",
      "  - science_q18_average_score: 100.00%\n",
      "  - science_q19_average_score: 100.00%\n",
      "  - reading_q13_total_timing: 100.00%\n",
      "  - reading_q14_total_timing: 100.00%\n",
      "  - reading_q15_total_timing: 100.00%\n",
      "  - math_q17_total_timing: 100.00%\n",
      "  - math_q18_total_timing: 100.00%\n",
      "\n",
      "Dont ordinales : 0\n",
      "Dont catÃ©gorielles : 1\n",
      "\n",
      "âš ï¸ Recommandation : Supprimer ces variables (trop peu d'information)\n"
     ]
    }
   ],
   "source": [
    "# 11.2 - Identifier variables avec trop de missing (>80%)\n",
    "# Ces variables sont candidates Ã  la suppression\n",
    "\n",
    "high_missing_vars = missing_stats.filter(pl.col('missing_pct') > 80)['variable'].to_list()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VARIABLES AVEC >80% DE VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nNombre total : {len(high_missing_vars)}\")\n",
    "print(f\"\\nPremiÃ¨res 20 variables :\")\n",
    "for var in high_missing_vars[:20]:\n",
    "    missing_pct = missing_stats.filter(pl.col('variable') == var)['missing_pct'][0]\n",
    "    print(f\"  - {var}: {missing_pct:.2f}%\")\n",
    "\n",
    "# VÃ©rifier si ce sont des variables ordinales/catÃ©gorielles\n",
    "high_missing_ordinal = [v for v in high_missing_vars if v in ordinal_vars]\n",
    "high_missing_categorical = [v for v in high_missing_vars if v in categorical_vars]\n",
    "\n",
    "print(f\"\\nDont ordinales : {len(high_missing_ordinal)}\")\n",
    "print(f\"Dont catÃ©gorielles : {len(high_missing_categorical)}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Recommandation : Supprimer ces variables (trop peu d'information)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_missing_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LANGTEST_PAQ']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_missing_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A supprimer: LANGTEST_PAQ (trop de NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRÃ‰LATIONS ENTRE PATTERNS DE MISSING\n",
      "============================================================\n",
      "\n",
      "Variables avec missing modÃ©rÃ© (20-80%) : 150\n",
      "\n",
      "Matrice de corrÃ©lation des patterns de missing (15 variables) :\n",
      "          MATHEASE  MISSSC  ST095  PA033  ST146  ST100  ST097  ST113  PA032  \\\n",
      "MATHEASE     1.000   0.803 -0.418 -0.418 -0.418 -0.418 -0.418 -0.418 -0.418   \n",
      "MISSSC       0.803   1.000 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428   \n",
      "ST095       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "PA033       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST146       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST100       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST097       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST113       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "PA032       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "IC001       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST011       -0.418  -0.428  1.000  1.000  1.000  1.000  1.000  1.000  1.000   \n",
      "ST307       -0.466  -0.477 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480   \n",
      "ST160       -0.466  -0.477 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480   \n",
      "ST168       -0.466  -0.477 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480   \n",
      "ST161       -0.466  -0.477 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480 -0.480   \n",
      "\n",
      "          IC001  ST011  ST307  ST160  ST168  ST161  \n",
      "MATHEASE -0.418 -0.418 -0.466 -0.466 -0.466 -0.466  \n",
      "MISSSC   -0.428 -0.428 -0.477 -0.477 -0.477 -0.477  \n",
      "ST095     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "PA033     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST146     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST100     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST097     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST113     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "PA032     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "IC001     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST011     1.000  1.000 -0.480 -0.480 -0.480 -0.480  \n",
      "ST307    -0.480 -0.480  1.000  1.000  1.000  1.000  \n",
      "ST160    -0.480 -0.480  1.000  1.000  1.000  1.000  \n",
      "ST168    -0.480 -0.480  1.000  1.000  1.000  1.000  \n",
      "ST161    -0.480 -0.480  1.000  1.000  1.000  1.000  \n",
      "\n",
      "âš ï¸ Paires avec patterns de missing corrÃ©lÃ©s (|r| > 0.5) :\n",
      "  - MATHEASE <-> MISSSC : 0.803\n",
      "  - ST095 <-> PA033 : 1.000\n",
      "  - ST095 <-> ST146 : 1.000\n",
      "  - ST095 <-> ST100 : 1.000\n",
      "  - ST095 <-> ST097 : 1.000\n",
      "  - ST095 <-> ST113 : 1.000\n",
      "  - ST095 <-> PA032 : 1.000\n",
      "  - ST095 <-> IC001 : 1.000\n",
      "  - ST095 <-> ST011 : 1.000\n",
      "  - PA033 <-> ST146 : 1.000\n",
      "\n",
      "â¡ï¸ Cela suggÃ¨re MAR/MNAR : certaines variables manquent ensemble\n"
     ]
    }
   ],
   "source": [
    "# 11.3 - Calculer corrÃ©lations entre patterns de missing (Ã©chantillon)\n",
    "# Pour dÃ©tecter si certaines variables manquent ensemble (indicateur de MAR/MNAR)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRÃ‰LATIONS ENTRE PATTERNS DE MISSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SÃ©lectionner variables avec missing modÃ©rÃ© (20-80%) pour analyse\n",
    "moderate_missing_vars = missing_stats.filter(\n",
    "    (pl.col('missing_pct') >= 20) & \n",
    "    (pl.col('missing_pct') <= 80)\n",
    ")['variable'].to_list()\n",
    "\n",
    "# Garder seulement variables ordinales/catÃ©gorielles\n",
    "moderate_missing_vars = [v for v in moderate_missing_vars if v in vars_of_interest]\n",
    "\n",
    "print(f\"\\nVariables avec missing modÃ©rÃ© (20-80%) : {len(moderate_missing_vars)}\")\n",
    "\n",
    "# Prendre un Ã©chantillon de 15 variables max pour la matrice de corrÃ©lation\n",
    "sample_missing_vars = moderate_missing_vars[:min(15, len(moderate_missing_vars))]\n",
    "\n",
    "if len(sample_missing_vars) > 1:\n",
    "    # CrÃ©er sous-matrice de missing pour ces variables\n",
    "    missing_sample = missing_matrix.select(sample_missing_vars).to_pandas()\n",
    "    \n",
    "    # Calculer corrÃ©lation entre patterns de missing\n",
    "    missing_corr = missing_sample.corr(method='pearson')\n",
    "    \n",
    "    print(f\"\\nMatrice de corrÃ©lation des patterns de missing ({len(sample_missing_vars)} variables) :\")\n",
    "    print(missing_corr.round(3))\n",
    "    \n",
    "    # Identifier paires fortement corrÃ©lÃ©es\n",
    "    strong_missing_corr = []\n",
    "    for i, var1 in enumerate(sample_missing_vars):\n",
    "        for j, var2 in enumerate(sample_missing_vars):\n",
    "            if i < j:\n",
    "                corr_val = missing_corr.loc[var1, var2]\n",
    "                if abs(corr_val) > 0.5:\n",
    "                    strong_missing_corr.append((var1, var2, corr_val))\n",
    "    \n",
    "    if strong_missing_corr:\n",
    "        print(f\"\\nâš ï¸ Paires avec patterns de missing corrÃ©lÃ©s (|r| > 0.5) :\")\n",
    "        for v1, v2, corr in strong_missing_corr[:10]:  # Top 10\n",
    "            print(f\"  - {v1} <-> {v2} : {corr:.3f}\")\n",
    "        print(\"\\nâ¡ï¸ Cela suggÃ¨re MAR/MNAR : certaines variables manquent ensemble\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Patterns de missing indÃ©pendants (suggÃ¨re MCAR)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Pas assez de variables avec missing modÃ©rÃ© pour analyser les corrÃ©lations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATÃ‰GIES D'IMPUTATION RECOMMANDÃ‰ES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Distribution des variables par stratÃ©gie :\n",
      "\n",
      "supprimer (>80% missing): 1 variables\n",
      "imputation complexe (50-80% missing): 118 variables\n",
      "imputation simple (20-50% missing): 35 variables\n",
      "imputation minimale (<20% missing): 4 variables\n",
      "pas de missing: 7 variables\n",
      "\n",
      "================================================================================\n",
      "RECOMMANDATIONS DÃ‰TAILLÃ‰ES\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ SUPPRIMER (1 variables)\n",
      "   Trop peu d'information pour Ãªtre utiles\n",
      "   Exemples : ['LANGTEST_PAQ']\n",
      "\n",
      "2ï¸âƒ£ IMPUTATION COMPLEXE (118 variables)\n",
      "   Options : KNN imputer, MICE, ou crÃ©er flag 'is_missing'\n",
      "   Exemples : ['ISCEDP', 'ST253', 'ST311', 'IC180', 'PA195']\n",
      "\n",
      "3ï¸âƒ£ IMPUTATION SIMPLE (35 variables)\n",
      "   Ordinales : mÃ©diane ou mode\n",
      "   CatÃ©gorielles : mode ou catÃ©gorie 'Unknown'\n",
      "   Exemples : ['IC171', 'ST038', 'ST255', 'ST034', 'IC173']\n",
      "\n",
      "4ï¸âƒ£ IMPUTATION MINIMALE (4 variables)\n",
      "   Mode pour catÃ©gorielles, mÃ©diane pour ordinales\n",
      "   Exemples : ['LANGTEST_QQQ', 'ST003D02T', 'ST003D03T', 'ST004D01T']\n",
      "\n",
      "5ï¸âƒ£ PAS DE MISSING (7 variables)\n",
      "   Aucune imputation nÃ©cessaire\n",
      "   Exemples : ['CYC', 'NatCen', 'SUBNATIO', 'OECD', 'ADMINMODE']\n"
     ]
    }
   ],
   "source": [
    "# 11.4 - StratÃ©gie d'imputation recommandÃ©e par variable\n",
    "print(\"=\" * 80)\n",
    "print(\"STRATÃ‰GIES D'IMPUTATION RECOMMANDÃ‰ES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CatÃ©goriser les variables par niveau de missing et proposer stratÃ©gies\n",
    "imputation_strategies = {\n",
    "    'supprimer (>80% missing)': [],\n",
    "    'imputation complexe (50-80% missing)': [],\n",
    "    'imputation simple (20-50% missing)': [],\n",
    "    'imputation minimale (<20% missing)': [],\n",
    "    'pas de missing': []\n",
    "}\n",
    "\n",
    "for var in vars_of_interest:\n",
    "    missing_pct = missing_stats.filter(pl.col('variable') == var)['missing_pct'][0]\n",
    "    \n",
    "    if missing_pct > 80:\n",
    "        imputation_strategies['supprimer (>80% missing)'].append(var)\n",
    "    elif missing_pct > 50:\n",
    "        imputation_strategies['imputation complexe (50-80% missing)'].append(var)\n",
    "    elif missing_pct > 20:\n",
    "        imputation_strategies['imputation simple (20-50% missing)'].append(var)\n",
    "    elif missing_pct > 0:\n",
    "        imputation_strategies['imputation minimale (<20% missing)'].append(var)\n",
    "    else:\n",
    "        imputation_strategies['pas de missing'].append(var)\n",
    "\n",
    "print(\"\\nğŸ“Š Distribution des variables par stratÃ©gie :\\n\")\n",
    "for strategy, vars_list in imputation_strategies.items():\n",
    "    print(f\"{strategy}: {len(vars_list)} variables\")\n",
    "\n",
    "# Recommandations dÃ©taillÃ©es\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMANDATIONS DÃ‰TAILLÃ‰ES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ SUPPRIMER ({len(imputation_strategies['supprimer (>80% missing)'])} variables)\")\n",
    "print(\"   Trop peu d'information pour Ãªtre utiles\")\n",
    "if imputation_strategies['supprimer (>80% missing)']:\n",
    "    print(f\"   Exemples : {imputation_strategies['supprimer (>80% missing)'][:5]}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ IMPUTATION COMPLEXE ({len(imputation_strategies['imputation complexe (50-80% missing)'])} variables)\")\n",
    "print(\"   Options : KNN imputer, MICE, ou crÃ©er flag 'is_missing'\")\n",
    "if imputation_strategies['imputation complexe (50-80% missing)']:\n",
    "    print(f\"   Exemples : {imputation_strategies['imputation complexe (50-80% missing)'][:5]}\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ IMPUTATION SIMPLE ({len(imputation_strategies['imputation simple (20-50% missing)'])} variables)\")\n",
    "print(\"   Ordinales : mÃ©diane ou mode\")\n",
    "print(\"   CatÃ©gorielles : mode ou catÃ©gorie 'Unknown'\")\n",
    "if imputation_strategies['imputation simple (20-50% missing)']:\n",
    "    print(f\"   Exemples : {imputation_strategies['imputation simple (20-50% missing)'][:5]}\")\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ IMPUTATION MINIMALE ({len(imputation_strategies['imputation minimale (<20% missing)'])} variables)\")\n",
    "print(\"   Mode pour catÃ©gorielles, mÃ©diane pour ordinales\")\n",
    "if imputation_strategies['imputation minimale (<20% missing)']:\n",
    "    print(f\"   Exemples : {imputation_strategies['imputation minimale (<20% missing)'][:5]}\")\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£ PAS DE MISSING ({len(imputation_strategies['pas de missing'])} variables)\")\n",
    "print(\"   Aucune imputation nÃ©cessaire\")\n",
    "if imputation_strategies['pas de missing']:\n",
    "    print(f\"   Exemples : {imputation_strategies['pas de missing'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "STRATÃ‰GIES D'IMPUTATION RECOMMANDÃ‰ES\n",
    "================================================================================\n",
    "\n",
    "ğŸ“Š Distribution des variables par stratÃ©gie :\n",
    "\n",
    "supprimer (>80% missing): 1 variables\n",
    "imputation complexe (50-80% missing): 118 variables\n",
    "imputation simple (20-50% missing): 35 variables\n",
    "imputation minimale (<20% missing): 4 variables\n",
    "pas de missing: 7 variables\n",
    "\n",
    "================================================================================\n",
    "RECOMMANDATIONS DÃ‰TAILLÃ‰ES\n",
    "================================================================================\n",
    "\n",
    "1ï¸âƒ£ SUPPRIMER (1 variables)\n",
    "   Trop peu d'information pour Ãªtre utiles\n",
    "   Exemples : ['LANGTEST_PAQ']\n",
    "\n",
    "2ï¸âƒ£ IMPUTATION COMPLEXE (118 variables)\n",
    "   Options : KNN imputer\n",
    "   Exemples : ['ISCEDP', 'ST253', 'ST311', 'IC180', 'PA195']\n",
    "\n",
    "3ï¸âƒ£ IMPUTATION SIMPLE (35 variables)\n",
    "   Ordinales : mÃ©diane ou mode\n",
    "   CatÃ©gorielles : mode ou catÃ©gorie 'Unknown'\n",
    "   Exemples : ['IC171', 'ST038', 'ST255', 'ST034', 'IC173']\n",
    "\n",
    "4ï¸âƒ£ IMPUTATION MINIMALE (4 variables)\n",
    "   Mode pour catÃ©gorielles, mÃ©diane pour ordinales\n",
    "   Exemples : ['LANGTEST_QQQ', 'ST003D02T', 'ST003D03T', 'ST004D01T']\n",
    "\n",
    "5ï¸âƒ£ PAS DE MISSING (7 variables)\n",
    "   Aucune imputation nÃ©cessaire\n",
    "   Exemples : ['CYC', 'NatCen', 'SUBNATIO', 'OECD', 'ADMINMODE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SynthÃ¨se finale et recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. SYNTHÃˆSE FINALE - Toutes les recommandations pour le preprocessing\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SYNTHÃˆSE FINALE - RECOMMANDATIONS POUR LE PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“Š STATISTIQUES GLOBALES\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset : {df.height:,} lignes Ã— {df.width} colonnes\")\n",
    "print(f\"Variables ordinales : {len(ordinal_vars)}\")\n",
    "print(f\"Variables catÃ©gorielles : {len(categorical_vars)}\")\n",
    "print(f\"Missing global : {missing_pct_overall:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ—‘ï¸  VARIABLES Ã€ SUPPRIMER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Combiner toutes les variables Ã  supprimer\n",
    "vars_to_remove = list(set(\n",
    "    vars_to_drop +  # Redondantes\n",
    "    imputation_strategies['supprimer (>80% missing)']  # Trop de missing\n",
    "))\n",
    "\n",
    "print(f\"Total : {len(vars_to_remove)} variables\")\n",
    "print(f\"  - Redondantes : {len(vars_to_drop)}\")\n",
    "print(f\"  - >80% missing : {len(imputation_strategies['supprimer (>80% missing)'])}\")\n",
    "print(f\"\\nExemples : {vars_to_remove[:10]}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ VARIABLES ORDINALES - Traitement\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total : {len(ordinal_vars)}\")\n",
    "print(f\"CardinalitÃ© moyenne : {ordinal_df['n_unique'].mean():.1f}\")\n",
    "print(f\"Missing moyen : {ordinal_df['missing_pct'].mean():.2f}%\")\n",
    "print(\"\\nRecommandations :\")\n",
    "print(\"  1. Encodage ordinal (LabelEncoder) pour prÃ©server l'ordre\")\n",
    "print(\"  2. Imputation selon niveau de missing (voir section 11)\")\n",
    "print(\"  3. Normalisation/standardisation si nÃ©cessaire\")\n",
    "\n",
    "print(\"\\nğŸ·ï¸  VARIABLES CATÃ‰GORIELLES - Traitement\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total : {len(categorical_vars)}\")\n",
    "print(f\"CardinalitÃ© moyenne : {categorical_df['cardinality'].mean():.1f}\")\n",
    "print(f\"Missing moyen : {categorical_df['missing_pct'].mean():.2f}%\")\n",
    "\n",
    "# Grouper par cardinalitÃ©\n",
    "cat_faible = categorical_df.filter(pl.col('cardinality_type') == 'faible').height\n",
    "cat_moyenne = categorical_df.filter(pl.col('cardinality_type') == 'moyenne').height\n",
    "cat_haute = categorical_df.filter(pl.col('cardinality_type') == 'haute').height\n",
    "\n",
    "print(f\"\\nDistribution :\")\n",
    "print(f\"  - Faible cardinalitÃ© (<10) : {cat_faible} â†’ One-Hot Encoding\")\n",
    "print(f\"  - Moyenne cardinalitÃ© (10-50) : {cat_moyenne} â†’ Target Encoding ou Frequency Encoding\")\n",
    "print(f\"  - Haute cardinalitÃ© (>50) : {cat_haute} â†’ Target Encoding + dimensionality reduction\")\n",
    "\n",
    "print(\"\\nâš ï¸  VARIABLES Ã€ HAUTE CARDINALITÃ‰ - Attention particuliÃ¨re\")\n",
    "print(\"-\" * 80)\n",
    "high_card_vars = categorical_df.filter(pl.col('cardinality') > 50)['variable'].to_list()\n",
    "print(f\"Variables concernÃ©es : {len(high_card_vars)}\")\n",
    "for var in high_card_vars:\n",
    "    card = categorical_df.filter(pl.col('variable') == var)['cardinality'][0]\n",
    "    print(f\"  - {var}: {card} catÃ©gories\")\n",
    "print(\"\\nRecommandations :\")\n",
    "print(\"  â€¢ STRATUM (2472 catÃ©gories) : Target encoding ou regroupement par patterns\")\n",
    "print(\"  â€¢ OCOD1/2/3 (600-700 catÃ©gories) : Target encoding ou regroupement ISCO\")\n",
    "print(\"  â€¢ Autres : Frequency encoding ou regroupement catÃ©gories rares\")\n",
    "\n",
    "print(\"\\nğŸ’¡ STRATÃ‰GIES D'IMPUTATION\")\n",
    "print(\"-\" * 80)\n",
    "for strategy, vars_list in imputation_strategies.items():\n",
    "    if strategy != 'supprimer (>80% missing)':\n",
    "        print(f\"{strategy}: {len(vars_list)} variables\")\n",
    "\n",
    "print(\"\\nâœ… PROCHAINES Ã‰TAPES\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. CrÃ©er classe OrdinalPreprocessor\")\n",
    "print(\"2. CrÃ©er classe CategoricalPreprocessor\") \n",
    "print(\"3. ImplÃ©menter mÃ©thodes d'imputation adaptatives\")\n",
    "print(\"4. ImplÃ©menter encodages par type de cardinalitÃ©\")\n",
    "print(\"5. CrÃ©er pipeline complet avec validation\")\n",
    "print(\"6. Tester sur X_train complet\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "SYNTHÃˆSE FINALE - RECOMMANDATIONS POUR LE PREPROCESSING\n",
    "================================================================================\n",
    "\n",
    "ğŸ“Š STATISTIQUES GLOBALES\n",
    "--------------------------------------------------------------------------------\n",
    "Dataset : 50,000 lignes Ã— 307 colonnes\n",
    "Variables ordinales : 95\n",
    "Variables catÃ©gorielles : 70\n",
    "Missing global : 53.92%\n",
    "\n",
    "ğŸ—‘ï¸  VARIABLES Ã€ SUPPRIMER\n",
    "--------------------------------------------------------------------------------\n",
    "Total : 1 variables\n",
    "  - Redondantes : 0\n",
    "  - >80% missing : 1\n",
    "\n",
    "Exemples : ['LANGTEST_PAQ']\n",
    "\n",
    "ğŸ“ˆ VARIABLES ORDINALES - Traitement\n",
    "--------------------------------------------------------------------------------\n",
    "Total : 95\n",
    "CardinalitÃ© moyenne : 27.1\n",
    "Missing moyen : 58.68%\n",
    "\n",
    "Recommandations :\n",
    "  1. Encodage ordinal (LabelEncoder) pour prÃ©server l'ordre\n",
    "  2. Imputation selon niveau de missing (voir section 11)\n",
    "  3. Normalisation/standardisation si nÃ©cessaire\n",
    "\n",
    "ğŸ·ï¸  VARIABLES CATÃ‰GORIELLES - Traitement\n",
    "--------------------------------------------------------------------------------\n",
    "Total : 70\n",
    "CardinalitÃ© moyenne : 48.7\n",
    "Missing moyen : 47.93%\n",
    "\n",
    "Distribution :\n",
    "  - Faible cardinalitÃ© (<10) : 31 â†’ One-Hot Encoding\n",
    "  - Moyenne cardinalitÃ© (10-50) : 28 â†’ Target Encoding ou Frequency Encoding\n",
    "  - Haute cardinalitÃ© (>50) : 11 â†’ Target Encoding + dimensionality reduction\n",
    "\n",
    "âš ï¸  VARIABLES Ã€ HAUTE CARDINALITÃ‰ - Attention particuliÃ¨re\n",
    "--------------------------------------------------------------------------------\n",
    "Variables concernÃ©es : 11\n",
    "  - NatCen: 96 catÃ©gories\n",
    "  - SUBNATIO: 123 catÃ©gories\n",
    "  - LANGTEST_QQQ: 61 catÃ©gories\n",
    "  - LANGTEST_COG: 62 catÃ©gories\n",
    "  - OCOD1: 665 catÃ©gories\n",
    "  - OCOD2: 707 catÃ©gories\n",
    "  - OCOD3: 589 catÃ©gories\n",
    "  - ST296: 143 catÃ©gories\n",
    "  - PA006: 107 catÃ©gories\n",
    "  - WB162: 114 catÃ©gories\n",
    "  - EC153: 74 catÃ©gories\n",
    "\n",
    "Recommandations :\n",
    "  â€¢ STRATUM (2472 catÃ©gories) : Target encoding ou regroupement par patterns\n",
    "  â€¢ OCOD1/2/3 (600-700 catÃ©gories) : Target encoding ou regroupement ISCO\n",
    "  â€¢ Autres : Frequency encoding ou regroupement catÃ©gories rares\n",
    "\n",
    "ğŸ’¡ STRATÃ‰GIES D'IMPUTATION\n",
    "--------------------------------------------------------------------------------\n",
    "imputation complexe (50-80% missing): 118 variables\n",
    "imputation simple (20-50% missing): 35 variables\n",
    "imputation minimale (<20% missing): 4 variables\n",
    "pas de missing: 7 variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvHickathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
